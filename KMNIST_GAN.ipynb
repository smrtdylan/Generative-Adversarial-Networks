{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlDSm2PSTEu3hOTAZb1E+i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "68p07jEJQlAL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install -U datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyQlo1RDSEAj",
        "outputId": "f6766d32-eb16-440a-d2a0-4b8ae7a10249"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "FgJEFyrHSFT6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = torchvision.datasets.KMNIST(root='./data', download = True, transform = transform, train=True)"
      ],
      "metadata": {
        "id": "Smxee5yKnvei",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6fd528-5d6b-4ed8-9a7a-bb37c9fdae16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18.2M/18.2M [00:13<00:00, 1.32MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 202kB/s]\n",
            "100%|██████████| 3.04M/3.04M [00:02<00:00, 1.47MB/s]\n",
            "100%|██████████| 5.12k/5.12k [00:00<00:00, 6.68MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader = DataLoader(train, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "oaz1BBCcu1iQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loader.batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osJMwI9USKk4",
        "outputId": "148dbf40-0ffd-481a-d928-d859cf58a764"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iter_train = iter(train_loader)\n",
        "\n",
        "batch = next(iter_train)\n",
        "\n",
        "image = batch[0][0]\n",
        "\n",
        "\n",
        "print(image.shape)\n",
        "\n",
        "np_img = image.numpy()\n",
        "\n",
        "np_img = np_img.transpose(1,2,0)\n",
        "\n",
        "plt.imshow(np_img, cmap = 'gray')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "Lx2nHhZBT2fz",
        "outputId": "470f43cb-5257-41da-e375-8ffce7e87cdd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "tensor(2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78fe75ac1250>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 128\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, batch_dim, latent_dim):\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            # 100 --> 128,7,7\n",
        "            nn.Linear(in_features = latent_dim, out_features = 128*7*7),\n",
        "            nn.BatchNorm1d(128*7*7),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(dim = 1, unflattened_size = (128,7,7)),\n",
        "\n",
        "            # 128,7,7 --> 64,14,14\n",
        "            nn.ConvTranspose2d(in_channels = 128, out_channels = 64, kernel_size = 4, stride=2, padding = 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # 64,14,14 --> 1,28,28\n",
        "            nn.ConvTranspose2d(in_channels = 64, out_channels = 1, kernel_size = 4, stride=2, padding = 1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "    def initialize_weights(self):\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, nn.ConvTranspose2d):\n",
        "          nn.init.kaiming_uniform_(m.weight)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "          nn.init.kaiming_uniform_(m.weight)"
      ],
      "metadata": {
        "id": "w2cYrjUvT3VV"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G = Generator(32, latent_dim).to('cpu')\n"
      ],
      "metadata": {
        "id": "pGL9IokbWpKD"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.randn(32, latent_dim)\n",
        "fake = G(z)\n",
        "\n",
        "print(fake.shape)\n",
        "\n",
        "\n",
        "fake_img = fake[1].detach().numpy()\n",
        "\n",
        "fake_img = fake_img.transpose(1,2,0)\n",
        "\n",
        "plt.imshow(fake_img, cmap='gray')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "lvpiYfbfWqEv",
        "outputId": "6c2d063a-18fc-4337-e560-2aecbfe1ce27"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1, 28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x78fe70437810>"
            ]
          },
          "metadata": {},
          "execution_count": 131
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.disc = nn.Sequential(\n",
        "            # 1,28,28 --> 32,14,14\n",
        "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=2, stride=2, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            # 32,14,14 --> 64,7,7\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2, stride=2, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "\n",
        "            # Last Layer\n",
        "            nn.Linear(in_features = 128*7*7, out_features = 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.disc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "hLxWhaDLWrEZ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = Discriminator().to('cpu')\n",
        "\n",
        "print(D(fake).shape)\n",
        "\n",
        "fake_probs = torch.argmax(D(fake))\n",
        "print(D(fake))\n",
        "print(fake_probs)\n",
        "\n",
        "\n",
        "for batch in train_loader:\n",
        "\n",
        "  images= batch[0]\n",
        "  print(f'inputs shape: {images.shape}')\n",
        "  images = images.type(torch.float)\n",
        "  print(f'after network: {D(images).shape}')\n",
        "\n",
        "  print()\n",
        "  print()\n",
        "\n",
        "  real_probs = torch.argmax(D(images))\n",
        "  real_input = D(images)\n",
        "  print(real_probs)\n",
        "\n",
        "  real_labels = torch.full_like(real_input, 0.95)\n",
        "\n",
        "  print(real_labels)\n",
        "  print(real_input.shape)\n",
        "\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk5XTn95WswU",
        "outputId": "4310276d-7e1b-4cbf-8edb-2a33b97652a1"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 1])\n",
            "tensor([[0.5733],\n",
            "        [0.6708],\n",
            "        [0.3914],\n",
            "        [0.3788],\n",
            "        [0.3829],\n",
            "        [0.3985],\n",
            "        [0.6383],\n",
            "        [0.4662],\n",
            "        [0.6345],\n",
            "        [0.5124],\n",
            "        [0.5506],\n",
            "        [0.4171],\n",
            "        [0.4598],\n",
            "        [0.4504],\n",
            "        [0.4663],\n",
            "        [0.5551],\n",
            "        [0.4255],\n",
            "        [0.4764],\n",
            "        [0.6340],\n",
            "        [0.6240],\n",
            "        [0.4374],\n",
            "        [0.4369],\n",
            "        [0.4675],\n",
            "        [0.3478],\n",
            "        [0.3333],\n",
            "        [0.4590],\n",
            "        [0.4857],\n",
            "        [0.5890],\n",
            "        [0.3545],\n",
            "        [0.4080],\n",
            "        [0.3552],\n",
            "        [0.7286]], grad_fn=<SigmoidBackward0>)\n",
            "tensor(31)\n",
            "inputs shape: torch.Size([32, 1, 28, 28])\n",
            "after network: torch.Size([32, 1])\n",
            "\n",
            "\n",
            "tensor(30)\n",
            "tensor([[0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500],\n",
            "        [0.9500]])\n",
            "torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g_params = 0\n",
        "d_params = 0\n",
        "\n",
        "# Discriminator Parameters\n",
        "for x in D.parameters():\n",
        "  d_params += len(torch.flatten(x))\n",
        "\n",
        "print(f'Discriminator parameters: {d_params:,}')\n",
        "\n",
        "for x in G.parameters():\n",
        "  g_params += len(torch.flatten(x))\n",
        "\n",
        "print(f'Generator parameters: {g_params:,}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jft4X4_xWtni",
        "outputId": "7b9b61cc-c5d2-4e26-935f-6cfa554f37a3"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator parameters: 39,873\n",
            "Generator parameters: 854,849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = nn.BCELoss()\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=0.0001)\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "bqrYTB0gYGQ0"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdvbAfRWYHTA",
        "outputId": "515354ca-4c25-404a-fa86-dbef6b38d919"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print(f'Epoch {epoch+1}')\n",
        "\n",
        "  G.train(True)\n",
        "  D.train(True)\n",
        "  DRunning_loss = 0.0\n",
        "  GRunning_loss = 0.0\n",
        "\n",
        "  for index, batch in enumerate(train_loader):\n",
        "    real_images = batch[0]\n",
        "    real_images = real_images.to(device)\n",
        "    real_images = real_images.type(torch.float)\n",
        "\n",
        "    batch_size = real_images.shape[0]\n",
        "\n",
        "    z = torch.randn(batch_size, latent_dim, device=device)\n",
        "    fake = G(z)\n",
        "\n",
        "    ### train discriminator ###\n",
        "\n",
        "    #discriminate real ones\n",
        "    D_optimizer.zero_grad()\n",
        "\n",
        "    D_real = D(real_images).view(-1)\n",
        "    D_loss_real = criterion(D_real, torch.full_like(D_real,0.90))\n",
        "\n",
        "    D_fake = D(fake.detach()).view(-1)\n",
        "    D_loss_fake = criterion(D_fake, torch.full_like(D_fake,0.10))\n",
        "\n",
        "    D_loss = (D_loss_real+ D_loss_fake) / 2\n",
        "\n",
        "\n",
        "    D.zero_grad()\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    DRunning_loss += D_loss.item()\n",
        "\n",
        "    ### train generator ###\n",
        "    G_optimizer.zero_grad()\n",
        "\n",
        "    output = D(fake)\n",
        "    G_loss = criterion(output, torch.ones_like(output))\n",
        "\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "\n",
        "    GRunning_loss+=G_loss.item()\n",
        "\n",
        "    if (index % 25 == 24):\n",
        "      print(f'Batch: {index+1}   D_Loss: {DRunning_loss/25:.3f}   G_Loss: {GRunning_loss/25:.3f}')\n",
        "      DRunning_loss = 0.0\n",
        "      GRunning_loss = 0.0\n",
        "\n",
        "with (open('G.pt', 'wb')) as f:\n",
        "  torch.save(G.state_dict(), f)\n",
        "\n",
        "with (open('D.pt', 'wb')) as f:\n",
        "  torch.save(D.state_dict(), f)\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdUH32RJYIcO",
        "outputId": "db2c8e87-4156-4e3e-ceba-272a65e7ddfd"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Batch: 25   D_Loss: 0.706   G_Loss: 0.708\n",
            "Batch: 50   D_Loss: 0.697   G_Loss: 0.723\n",
            "Batch: 75   D_Loss: 0.692   G_Loss: 0.721\n",
            "Batch: 100   D_Loss: 0.686   G_Loss: 0.726\n",
            "Batch: 125   D_Loss: 0.682   G_Loss: 0.734\n",
            "Batch: 150   D_Loss: 0.678   G_Loss: 0.739\n",
            "Batch: 175   D_Loss: 0.673   G_Loss: 0.748\n",
            "Batch: 200   D_Loss: 0.669   G_Loss: 0.759\n",
            "Batch: 225   D_Loss: 0.662   G_Loss: 0.768\n",
            "Batch: 250   D_Loss: 0.659   G_Loss: 0.776\n",
            "Batch: 275   D_Loss: 0.651   G_Loss: 0.793\n",
            "Batch: 300   D_Loss: 0.646   G_Loss: 0.802\n",
            "Batch: 325   D_Loss: 0.644   G_Loss: 0.814\n",
            "Batch: 350   D_Loss: 0.642   G_Loss: 0.823\n",
            "Batch: 375   D_Loss: 0.633   G_Loss: 0.838\n",
            "Batch: 400   D_Loss: 0.627   G_Loss: 0.851\n",
            "Batch: 425   D_Loss: 0.621   G_Loss: 0.864\n",
            "Batch: 450   D_Loss: 0.625   G_Loss: 0.869\n",
            "Batch: 475   D_Loss: 0.619   G_Loss: 0.887\n",
            "Batch: 500   D_Loss: 0.615   G_Loss: 0.886\n",
            "Batch: 525   D_Loss: 0.606   G_Loss: 0.907\n",
            "Batch: 550   D_Loss: 0.605   G_Loss: 0.924\n",
            "Batch: 575   D_Loss: 0.601   G_Loss: 0.931\n",
            "Batch: 600   D_Loss: 0.587   G_Loss: 0.950\n",
            "Batch: 625   D_Loss: 0.588   G_Loss: 0.967\n",
            "Batch: 650   D_Loss: 0.585   G_Loss: 0.970\n",
            "Batch: 675   D_Loss: 0.573   G_Loss: 0.983\n",
            "Batch: 700   D_Loss: 0.573   G_Loss: 1.002\n",
            "Batch: 725   D_Loss: 0.568   G_Loss: 1.021\n",
            "Batch: 750   D_Loss: 0.559   G_Loss: 1.022\n",
            "Batch: 775   D_Loss: 0.550   G_Loss: 1.056\n",
            "Batch: 800   D_Loss: 0.551   G_Loss: 1.059\n",
            "Batch: 825   D_Loss: 0.542   G_Loss: 1.063\n",
            "Batch: 850   D_Loss: 0.541   G_Loss: 1.084\n",
            "Batch: 875   D_Loss: 0.542   G_Loss: 1.072\n",
            "Batch: 900   D_Loss: 0.527   G_Loss: 1.110\n",
            "Batch: 925   D_Loss: 0.532   G_Loss: 1.108\n",
            "Batch: 950   D_Loss: 0.524   G_Loss: 1.123\n",
            "Batch: 975   D_Loss: 0.532   G_Loss: 1.108\n",
            "Batch: 1000   D_Loss: 0.536   G_Loss: 1.098\n",
            "Batch: 1025   D_Loss: 0.546   G_Loss: 1.126\n",
            "Batch: 1050   D_Loss: 0.527   G_Loss: 1.147\n",
            "Batch: 1075   D_Loss: 0.539   G_Loss: 1.223\n",
            "Batch: 1100   D_Loss: 0.530   G_Loss: 1.239\n",
            "Batch: 1125   D_Loss: 0.526   G_Loss: 1.253\n",
            "Batch: 1150   D_Loss: 0.534   G_Loss: 1.231\n",
            "Batch: 1175   D_Loss: 0.503   G_Loss: 1.322\n",
            "Batch: 1200   D_Loss: 0.502   G_Loss: 1.240\n",
            "Batch: 1225   D_Loss: 0.490   G_Loss: 1.246\n",
            "Batch: 1250   D_Loss: 0.474   G_Loss: 1.272\n",
            "Batch: 1275   D_Loss: 0.471   G_Loss: 1.295\n",
            "Batch: 1300   D_Loss: 0.465   G_Loss: 1.288\n",
            "Batch: 1325   D_Loss: 0.465   G_Loss: 1.324\n",
            "Batch: 1350   D_Loss: 0.482   G_Loss: 1.330\n",
            "Batch: 1375   D_Loss: 0.486   G_Loss: 1.375\n",
            "Batch: 1400   D_Loss: 0.493   G_Loss: 1.345\n",
            "Batch: 1425   D_Loss: 0.472   G_Loss: 1.470\n",
            "Batch: 1450   D_Loss: 0.458   G_Loss: 1.466\n",
            "Batch: 1475   D_Loss: 0.486   G_Loss: 1.431\n",
            "Batch: 1500   D_Loss: 0.457   G_Loss: 1.449\n",
            "Batch: 1525   D_Loss: 0.445   G_Loss: 1.468\n",
            "Batch: 1550   D_Loss: 0.439   G_Loss: 1.443\n",
            "Batch: 1575   D_Loss: 0.425   G_Loss: 1.482\n",
            "Batch: 1600   D_Loss: 0.417   G_Loss: 1.503\n",
            "Batch: 1625   D_Loss: 0.417   G_Loss: 1.492\n",
            "Batch: 1650   D_Loss: 0.416   G_Loss: 1.525\n",
            "Batch: 1675   D_Loss: 0.418   G_Loss: 1.462\n",
            "Batch: 1700   D_Loss: 0.399   G_Loss: 1.542\n",
            "Batch: 1725   D_Loss: 0.417   G_Loss: 1.553\n",
            "Batch: 1750   D_Loss: 0.409   G_Loss: 1.530\n",
            "Batch: 1775   D_Loss: 0.402   G_Loss: 1.568\n",
            "Batch: 1800   D_Loss: 0.404   G_Loss: 1.605\n",
            "Batch: 1825   D_Loss: 0.403   G_Loss: 1.552\n",
            "Batch: 1850   D_Loss: 0.398   G_Loss: 1.617\n",
            "Batch: 1875   D_Loss: 0.402   G_Loss: 1.577\n",
            "Epoch 2\n",
            "Batch: 25   D_Loss: 0.391   G_Loss: 1.651\n",
            "Batch: 50   D_Loss: 0.384   G_Loss: 1.683\n",
            "Batch: 75   D_Loss: 0.390   G_Loss: 1.691\n",
            "Batch: 100   D_Loss: 0.388   G_Loss: 1.657\n",
            "Batch: 125   D_Loss: 0.380   G_Loss: 1.699\n",
            "Batch: 150   D_Loss: 0.379   G_Loss: 1.706\n",
            "Batch: 175   D_Loss: 0.371   G_Loss: 1.774\n",
            "Batch: 200   D_Loss: 0.372   G_Loss: 1.772\n",
            "Batch: 225   D_Loss: 0.373   G_Loss: 1.787\n",
            "Batch: 250   D_Loss: 0.368   G_Loss: 1.817\n",
            "Batch: 275   D_Loss: 0.370   G_Loss: 1.810\n",
            "Batch: 300   D_Loss: 0.366   G_Loss: 1.823\n",
            "Batch: 325   D_Loss: 0.392   G_Loss: 1.798\n",
            "Batch: 350   D_Loss: 0.453   G_Loss: 1.786\n",
            "Batch: 375   D_Loss: 0.439   G_Loss: 1.725\n",
            "Batch: 400   D_Loss: 0.422   G_Loss: 1.884\n",
            "Batch: 425   D_Loss: 0.430   G_Loss: 1.832\n",
            "Batch: 450   D_Loss: 0.420   G_Loss: 1.833\n",
            "Batch: 475   D_Loss: 0.403   G_Loss: 1.855\n",
            "Batch: 500   D_Loss: 0.405   G_Loss: 1.912\n",
            "Batch: 525   D_Loss: 0.389   G_Loss: 1.830\n",
            "Batch: 550   D_Loss: 0.400   G_Loss: 1.844\n",
            "Batch: 575   D_Loss: 0.368   G_Loss: 1.904\n",
            "Batch: 600   D_Loss: 0.369   G_Loss: 1.920\n",
            "Batch: 625   D_Loss: 0.364   G_Loss: 1.968\n",
            "Batch: 650   D_Loss: 0.355   G_Loss: 2.010\n",
            "Batch: 675   D_Loss: 0.359   G_Loss: 2.013\n",
            "Batch: 700   D_Loss: 0.363   G_Loss: 2.038\n",
            "Batch: 725   D_Loss: 0.377   G_Loss: 1.884\n",
            "Batch: 750   D_Loss: 0.381   G_Loss: 1.992\n",
            "Batch: 775   D_Loss: 0.373   G_Loss: 1.913\n",
            "Batch: 800   D_Loss: 0.382   G_Loss: 1.848\n",
            "Batch: 825   D_Loss: 0.377   G_Loss: 1.869\n",
            "Batch: 850   D_Loss: 0.369   G_Loss: 1.883\n",
            "Batch: 875   D_Loss: 0.371   G_Loss: 1.943\n",
            "Batch: 900   D_Loss: 0.377   G_Loss: 1.880\n",
            "Batch: 925   D_Loss: 0.373   G_Loss: 1.853\n",
            "Batch: 950   D_Loss: 0.380   G_Loss: 1.907\n",
            "Batch: 975   D_Loss: 0.381   G_Loss: 1.879\n",
            "Batch: 1000   D_Loss: 0.389   G_Loss: 2.013\n",
            "Batch: 1025   D_Loss: 0.370   G_Loss: 2.087\n",
            "Batch: 1050   D_Loss: 0.371   G_Loss: 2.107\n",
            "Batch: 1075   D_Loss: 0.393   G_Loss: 1.961\n",
            "Batch: 1100   D_Loss: 0.392   G_Loss: 1.941\n",
            "Batch: 1125   D_Loss: 0.368   G_Loss: 1.988\n",
            "Batch: 1150   D_Loss: 0.367   G_Loss: 1.993\n",
            "Batch: 1175   D_Loss: 0.398   G_Loss: 1.953\n",
            "Batch: 1200   D_Loss: 0.368   G_Loss: 1.900\n",
            "Batch: 1225   D_Loss: 0.365   G_Loss: 1.989\n",
            "Batch: 1250   D_Loss: 0.349   G_Loss: 2.078\n",
            "Batch: 1275   D_Loss: 0.353   G_Loss: 2.058\n",
            "Batch: 1300   D_Loss: 0.349   G_Loss: 2.123\n",
            "Batch: 1325   D_Loss: 0.399   G_Loss: 2.088\n",
            "Batch: 1350   D_Loss: 0.348   G_Loss: 2.106\n",
            "Batch: 1375   D_Loss: 0.344   G_Loss: 2.100\n",
            "Batch: 1400   D_Loss: 0.342   G_Loss: 2.097\n",
            "Batch: 1425   D_Loss: 0.345   G_Loss: 2.084\n",
            "Batch: 1450   D_Loss: 0.352   G_Loss: 2.192\n",
            "Batch: 1475   D_Loss: 0.369   G_Loss: 2.123\n",
            "Batch: 1500   D_Loss: 0.359   G_Loss: 2.087\n",
            "Batch: 1525   D_Loss: 0.356   G_Loss: 2.136\n",
            "Batch: 1550   D_Loss: 0.357   G_Loss: 2.059\n",
            "Batch: 1575   D_Loss: 0.353   G_Loss: 2.004\n",
            "Batch: 1600   D_Loss: 0.365   G_Loss: 1.831\n",
            "Batch: 1625   D_Loss: 0.382   G_Loss: 1.892\n",
            "Batch: 1650   D_Loss: 0.383   G_Loss: 1.960\n",
            "Batch: 1675   D_Loss: 0.366   G_Loss: 2.027\n",
            "Batch: 1700   D_Loss: 0.355   G_Loss: 2.049\n",
            "Batch: 1725   D_Loss: 0.355   G_Loss: 2.051\n",
            "Batch: 1750   D_Loss: 0.344   G_Loss: 2.057\n",
            "Batch: 1775   D_Loss: 0.344   G_Loss: 2.132\n",
            "Batch: 1800   D_Loss: 0.345   G_Loss: 2.123\n",
            "Batch: 1825   D_Loss: 0.341   G_Loss: 2.103\n",
            "Batch: 1850   D_Loss: 0.341   G_Loss: 2.161\n",
            "Batch: 1875   D_Loss: 0.341   G_Loss: 2.131\n",
            "Epoch 3\n",
            "Batch: 25   D_Loss: 0.343   G_Loss: 2.135\n",
            "Batch: 50   D_Loss: 0.342   G_Loss: 2.095\n",
            "Batch: 75   D_Loss: 0.337   G_Loss: 2.161\n",
            "Batch: 100   D_Loss: 0.336   G_Loss: 2.177\n",
            "Batch: 125   D_Loss: 0.354   G_Loss: 2.196\n",
            "Batch: 150   D_Loss: 0.351   G_Loss: 1.973\n",
            "Batch: 175   D_Loss: 0.343   G_Loss: 2.083\n",
            "Batch: 200   D_Loss: 0.340   G_Loss: 2.193\n",
            "Batch: 225   D_Loss: 0.340   G_Loss: 2.158\n",
            "Batch: 250   D_Loss: 0.337   G_Loss: 2.199\n",
            "Batch: 275   D_Loss: 0.340   G_Loss: 2.207\n",
            "Batch: 300   D_Loss: 0.349   G_Loss: 2.190\n",
            "Batch: 325   D_Loss: 0.354   G_Loss: 2.167\n",
            "Batch: 350   D_Loss: 0.354   G_Loss: 2.140\n",
            "Batch: 375   D_Loss: 0.356   G_Loss: 2.093\n",
            "Batch: 400   D_Loss: 0.353   G_Loss: 2.118\n",
            "Batch: 425   D_Loss: 0.356   G_Loss: 2.116\n",
            "Batch: 450   D_Loss: 0.355   G_Loss: 2.102\n",
            "Batch: 475   D_Loss: 0.350   G_Loss: 2.141\n",
            "Batch: 500   D_Loss: 0.353   G_Loss: 2.143\n",
            "Batch: 525   D_Loss: 0.354   G_Loss: 2.079\n",
            "Batch: 550   D_Loss: 0.356   G_Loss: 2.050\n",
            "Batch: 575   D_Loss: 0.365   G_Loss: 2.037\n",
            "Batch: 600   D_Loss: 0.366   G_Loss: 1.955\n",
            "Batch: 625   D_Loss: 0.348   G_Loss: 2.026\n",
            "Batch: 650   D_Loss: 0.345   G_Loss: 2.089\n",
            "Batch: 675   D_Loss: 0.345   G_Loss: 2.115\n",
            "Batch: 700   D_Loss: 0.344   G_Loss: 2.115\n",
            "Batch: 725   D_Loss: 0.346   G_Loss: 2.137\n",
            "Batch: 750   D_Loss: 0.341   G_Loss: 2.174\n",
            "Batch: 775   D_Loss: 0.344   G_Loss: 2.186\n",
            "Batch: 800   D_Loss: 0.344   G_Loss: 2.153\n",
            "Batch: 825   D_Loss: 0.343   G_Loss: 2.128\n",
            "Batch: 850   D_Loss: 0.349   G_Loss: 2.129\n",
            "Batch: 875   D_Loss: 0.355   G_Loss: 2.148\n",
            "Batch: 900   D_Loss: 0.352   G_Loss: 2.008\n",
            "Batch: 925   D_Loss: 0.348   G_Loss: 2.072\n",
            "Batch: 950   D_Loss: 0.345   G_Loss: 2.139\n",
            "Batch: 975   D_Loss: 0.346   G_Loss: 2.140\n",
            "Batch: 1000   D_Loss: 0.341   G_Loss: 2.147\n",
            "Batch: 1025   D_Loss: 0.344   G_Loss: 2.202\n",
            "Batch: 1050   D_Loss: 0.347   G_Loss: 2.133\n",
            "Batch: 1075   D_Loss: 0.349   G_Loss: 2.127\n",
            "Batch: 1100   D_Loss: 0.340   G_Loss: 2.195\n",
            "Batch: 1125   D_Loss: 0.344   G_Loss: 2.190\n",
            "Batch: 1150   D_Loss: 0.345   G_Loss: 2.138\n",
            "Batch: 1175   D_Loss: 0.342   G_Loss: 2.165\n",
            "Batch: 1200   D_Loss: 0.340   G_Loss: 2.229\n",
            "Batch: 1225   D_Loss: 0.351   G_Loss: 2.201\n",
            "Batch: 1250   D_Loss: 0.350   G_Loss: 2.091\n",
            "Batch: 1275   D_Loss: 0.345   G_Loss: 2.153\n",
            "Batch: 1300   D_Loss: 0.347   G_Loss: 2.162\n",
            "Batch: 1325   D_Loss: 0.341   G_Loss: 2.189\n",
            "Batch: 1350   D_Loss: 0.340   G_Loss: 2.214\n",
            "Batch: 1375   D_Loss: 0.339   G_Loss: 2.202\n",
            "Batch: 1400   D_Loss: 0.340   G_Loss: 2.239\n",
            "Batch: 1425   D_Loss: 0.339   G_Loss: 2.204\n",
            "Batch: 1450   D_Loss: 0.341   G_Loss: 2.279\n",
            "Batch: 1475   D_Loss: 0.341   G_Loss: 2.209\n",
            "Batch: 1500   D_Loss: 0.339   G_Loss: 2.212\n",
            "Batch: 1525   D_Loss: 0.337   G_Loss: 2.269\n",
            "Batch: 1550   D_Loss: 0.338   G_Loss: 2.227\n",
            "Batch: 1575   D_Loss: 0.341   G_Loss: 2.217\n",
            "Batch: 1600   D_Loss: 0.347   G_Loss: 2.159\n",
            "Batch: 1625   D_Loss: 0.350   G_Loss: 2.167\n",
            "Batch: 1650   D_Loss: 0.342   G_Loss: 2.175\n",
            "Batch: 1675   D_Loss: 0.340   G_Loss: 2.173\n",
            "Batch: 1700   D_Loss: 0.338   G_Loss: 2.232\n",
            "Batch: 1725   D_Loss: 0.337   G_Loss: 2.246\n",
            "Batch: 1750   D_Loss: 0.342   G_Loss: 2.220\n",
            "Batch: 1775   D_Loss: 0.339   G_Loss: 2.204\n",
            "Batch: 1800   D_Loss: 0.339   G_Loss: 2.212\n",
            "Batch: 1825   D_Loss: 0.341   G_Loss: 2.229\n",
            "Batch: 1850   D_Loss: 0.339   G_Loss: 2.268\n",
            "Batch: 1875   D_Loss: 0.338   G_Loss: 2.225\n",
            "Epoch 4\n",
            "Batch: 25   D_Loss: 0.337   G_Loss: 2.289\n",
            "Batch: 50   D_Loss: 0.337   G_Loss: 2.307\n",
            "Batch: 75   D_Loss: 0.337   G_Loss: 2.280\n",
            "Batch: 100   D_Loss: 0.336   G_Loss: 2.288\n",
            "Batch: 125   D_Loss: 0.336   G_Loss: 2.278\n",
            "Batch: 150   D_Loss: 0.335   G_Loss: 2.270\n",
            "Batch: 175   D_Loss: 0.335   G_Loss: 2.287\n",
            "Batch: 200   D_Loss: 0.336   G_Loss: 2.308\n",
            "Batch: 225   D_Loss: 0.335   G_Loss: 2.275\n",
            "Batch: 250   D_Loss: 0.336   G_Loss: 2.316\n",
            "Batch: 275   D_Loss: 0.336   G_Loss: 2.298\n",
            "Batch: 300   D_Loss: 0.336   G_Loss: 2.270\n",
            "Batch: 325   D_Loss: 0.336   G_Loss: 2.241\n",
            "Batch: 350   D_Loss: 0.335   G_Loss: 2.271\n",
            "Batch: 375   D_Loss: 0.335   G_Loss: 2.297\n",
            "Batch: 400   D_Loss: 0.337   G_Loss: 2.292\n",
            "Batch: 425   D_Loss: 0.334   G_Loss: 2.304\n",
            "Batch: 450   D_Loss: 0.336   G_Loss: 2.295\n",
            "Batch: 475   D_Loss: 0.336   G_Loss: 2.267\n",
            "Batch: 500   D_Loss: 0.338   G_Loss: 2.247\n",
            "Batch: 525   D_Loss: 0.335   G_Loss: 2.275\n",
            "Batch: 550   D_Loss: 0.335   G_Loss: 2.300\n",
            "Batch: 575   D_Loss: 0.335   G_Loss: 2.260\n",
            "Batch: 600   D_Loss: 0.347   G_Loss: 2.215\n",
            "Batch: 625   D_Loss: 0.346   G_Loss: 2.182\n",
            "Batch: 650   D_Loss: 0.338   G_Loss: 2.226\n",
            "Batch: 675   D_Loss: 0.338   G_Loss: 2.296\n",
            "Batch: 700   D_Loss: 0.343   G_Loss: 2.189\n",
            "Batch: 725   D_Loss: 0.336   G_Loss: 2.296\n",
            "Batch: 750   D_Loss: 0.338   G_Loss: 2.304\n",
            "Batch: 775   D_Loss: 0.337   G_Loss: 2.277\n",
            "Batch: 800   D_Loss: 0.338   G_Loss: 2.241\n",
            "Batch: 825   D_Loss: 0.337   G_Loss: 2.260\n",
            "Batch: 850   D_Loss: 0.339   G_Loss: 2.247\n",
            "Batch: 875   D_Loss: 0.338   G_Loss: 2.271\n",
            "Batch: 900   D_Loss: 0.338   G_Loss: 2.262\n",
            "Batch: 925   D_Loss: 0.337   G_Loss: 2.264\n",
            "Batch: 950   D_Loss: 0.337   G_Loss: 2.240\n",
            "Batch: 975   D_Loss: 0.336   G_Loss: 2.242\n",
            "Batch: 1000   D_Loss: 0.337   G_Loss: 2.277\n",
            "Batch: 1025   D_Loss: 0.334   G_Loss: 2.320\n",
            "Batch: 1050   D_Loss: 0.337   G_Loss: 2.321\n",
            "Batch: 1075   D_Loss: 0.336   G_Loss: 2.272\n",
            "Batch: 1100   D_Loss: 0.335   G_Loss: 2.279\n",
            "Batch: 1125   D_Loss: 0.335   G_Loss: 2.277\n",
            "Batch: 1150   D_Loss: 0.341   G_Loss: 2.246\n",
            "Batch: 1175   D_Loss: 0.337   G_Loss: 2.238\n",
            "Batch: 1200   D_Loss: 0.337   G_Loss: 2.269\n",
            "Batch: 1225   D_Loss: 0.335   G_Loss: 2.255\n",
            "Batch: 1250   D_Loss: 0.336   G_Loss: 2.274\n",
            "Batch: 1275   D_Loss: 0.336   G_Loss: 2.297\n",
            "Batch: 1300   D_Loss: 0.333   G_Loss: 2.290\n",
            "Batch: 1325   D_Loss: 0.334   G_Loss: 2.272\n",
            "Batch: 1350   D_Loss: 0.340   G_Loss: 2.255\n",
            "Batch: 1375   D_Loss: 0.339   G_Loss: 2.262\n",
            "Batch: 1400   D_Loss: 0.346   G_Loss: 2.231\n",
            "Batch: 1425   D_Loss: 0.340   G_Loss: 2.244\n",
            "Batch: 1450   D_Loss: 0.339   G_Loss: 2.257\n",
            "Batch: 1475   D_Loss: 0.341   G_Loss: 2.159\n",
            "Batch: 1500   D_Loss: 0.336   G_Loss: 2.246\n",
            "Batch: 1525   D_Loss: 0.335   G_Loss: 2.291\n",
            "Batch: 1550   D_Loss: 0.334   G_Loss: 2.302\n",
            "Batch: 1575   D_Loss: 0.334   G_Loss: 2.317\n",
            "Batch: 1600   D_Loss: 0.338   G_Loss: 2.262\n",
            "Batch: 1625   D_Loss: 0.336   G_Loss: 2.226\n",
            "Batch: 1650   D_Loss: 0.334   G_Loss: 2.287\n",
            "Batch: 1675   D_Loss: 0.336   G_Loss: 2.268\n",
            "Batch: 1700   D_Loss: 0.335   G_Loss: 2.274\n",
            "Batch: 1725   D_Loss: 0.337   G_Loss: 2.283\n",
            "Batch: 1750   D_Loss: 0.336   G_Loss: 2.260\n",
            "Batch: 1775   D_Loss: 0.335   G_Loss: 2.256\n",
            "Batch: 1800   D_Loss: 0.336   G_Loss: 2.256\n",
            "Batch: 1825   D_Loss: 0.334   G_Loss: 2.255\n",
            "Batch: 1850   D_Loss: 0.333   G_Loss: 2.338\n",
            "Batch: 1875   D_Loss: 0.335   G_Loss: 2.267\n",
            "Epoch 5\n",
            "Batch: 25   D_Loss: 0.334   G_Loss: 2.244\n",
            "Batch: 50   D_Loss: 0.333   G_Loss: 2.356\n",
            "Batch: 75   D_Loss: 0.334   G_Loss: 2.347\n",
            "Batch: 100   D_Loss: 0.335   G_Loss: 2.305\n",
            "Batch: 125   D_Loss: 0.333   G_Loss: 2.264\n",
            "Batch: 150   D_Loss: 0.334   G_Loss: 2.275\n",
            "Batch: 175   D_Loss: 0.333   G_Loss: 2.280\n",
            "Batch: 200   D_Loss: 0.338   G_Loss: 2.251\n",
            "Batch: 225   D_Loss: 0.337   G_Loss: 2.284\n",
            "Batch: 250   D_Loss: 0.337   G_Loss: 2.206\n",
            "Batch: 275   D_Loss: 0.336   G_Loss: 2.279\n",
            "Batch: 300   D_Loss: 0.334   G_Loss: 2.247\n",
            "Batch: 325   D_Loss: 0.334   G_Loss: 2.273\n",
            "Batch: 350   D_Loss: 0.334   G_Loss: 2.307\n",
            "Batch: 375   D_Loss: 0.334   G_Loss: 2.293\n",
            "Batch: 400   D_Loss: 0.335   G_Loss: 2.267\n",
            "Batch: 425   D_Loss: 0.340   G_Loss: 2.231\n",
            "Batch: 450   D_Loss: 0.338   G_Loss: 2.256\n",
            "Batch: 475   D_Loss: 0.345   G_Loss: 2.237\n",
            "Batch: 500   D_Loss: 0.342   G_Loss: 2.182\n",
            "Batch: 525   D_Loss: 0.336   G_Loss: 2.277\n",
            "Batch: 550   D_Loss: 0.334   G_Loss: 2.304\n",
            "Batch: 575   D_Loss: 0.333   G_Loss: 2.275\n",
            "Batch: 600   D_Loss: 0.335   G_Loss: 2.287\n",
            "Batch: 625   D_Loss: 0.335   G_Loss: 2.246\n",
            "Batch: 650   D_Loss: 0.334   G_Loss: 2.307\n",
            "Batch: 675   D_Loss: 0.337   G_Loss: 2.250\n",
            "Batch: 700   D_Loss: 0.333   G_Loss: 2.290\n",
            "Batch: 725   D_Loss: 0.333   G_Loss: 2.272\n",
            "Batch: 750   D_Loss: 0.336   G_Loss: 2.275\n",
            "Batch: 775   D_Loss: 0.335   G_Loss: 2.271\n",
            "Batch: 800   D_Loss: 0.333   G_Loss: 2.289\n",
            "Batch: 825   D_Loss: 0.334   G_Loss: 2.291\n",
            "Batch: 850   D_Loss: 0.336   G_Loss: 2.242\n",
            "Batch: 875   D_Loss: 0.335   G_Loss: 2.247\n",
            "Batch: 900   D_Loss: 0.334   G_Loss: 2.260\n",
            "Batch: 925   D_Loss: 0.335   G_Loss: 2.271\n",
            "Batch: 950   D_Loss: 0.332   G_Loss: 2.313\n",
            "Batch: 975   D_Loss: 0.333   G_Loss: 2.311\n",
            "Batch: 1000   D_Loss: 0.333   G_Loss: 2.268\n",
            "Batch: 1025   D_Loss: 0.335   G_Loss: 2.298\n",
            "Batch: 1050   D_Loss: 0.336   G_Loss: 2.240\n",
            "Batch: 1075   D_Loss: 0.332   G_Loss: 2.305\n",
            "Batch: 1100   D_Loss: 0.336   G_Loss: 2.305\n",
            "Batch: 1125   D_Loss: 0.339   G_Loss: 2.237\n",
            "Batch: 1150   D_Loss: 0.335   G_Loss: 2.240\n",
            "Batch: 1175   D_Loss: 0.333   G_Loss: 2.267\n",
            "Batch: 1200   D_Loss: 0.333   G_Loss: 2.316\n",
            "Batch: 1225   D_Loss: 0.333   G_Loss: 2.316\n",
            "Batch: 1250   D_Loss: 0.333   G_Loss: 2.281\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.284\n",
            "Batch: 1300   D_Loss: 0.332   G_Loss: 2.316\n",
            "Batch: 1325   D_Loss: 0.333   G_Loss: 2.283\n",
            "Batch: 1350   D_Loss: 0.333   G_Loss: 2.304\n",
            "Batch: 1375   D_Loss: 0.334   G_Loss: 2.325\n",
            "Batch: 1400   D_Loss: 0.337   G_Loss: 2.249\n",
            "Batch: 1425   D_Loss: 0.337   G_Loss: 2.239\n",
            "Batch: 1450   D_Loss: 0.333   G_Loss: 2.266\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.316\n",
            "Batch: 1500   D_Loss: 0.336   G_Loss: 2.247\n",
            "Batch: 1525   D_Loss: 0.337   G_Loss: 2.283\n",
            "Batch: 1550   D_Loss: 0.334   G_Loss: 2.249\n",
            "Batch: 1575   D_Loss: 0.333   G_Loss: 2.315\n",
            "Batch: 1600   D_Loss: 0.331   G_Loss: 2.321\n",
            "Batch: 1625   D_Loss: 0.332   G_Loss: 2.272\n",
            "Batch: 1650   D_Loss: 0.332   G_Loss: 2.333\n",
            "Batch: 1675   D_Loss: 0.332   G_Loss: 2.304\n",
            "Batch: 1700   D_Loss: 0.332   G_Loss: 2.294\n",
            "Batch: 1725   D_Loss: 0.332   G_Loss: 2.301\n",
            "Batch: 1750   D_Loss: 0.334   G_Loss: 2.260\n",
            "Batch: 1775   D_Loss: 0.332   G_Loss: 2.307\n",
            "Batch: 1800   D_Loss: 0.333   G_Loss: 2.298\n",
            "Batch: 1825   D_Loss: 0.331   G_Loss: 2.330\n",
            "Batch: 1850   D_Loss: 0.332   G_Loss: 2.291\n",
            "Batch: 1875   D_Loss: 0.333   G_Loss: 2.290\n",
            "Epoch 6\n",
            "Batch: 25   D_Loss: 0.333   G_Loss: 2.271\n",
            "Batch: 50   D_Loss: 0.338   G_Loss: 2.268\n",
            "Batch: 75   D_Loss: 0.335   G_Loss: 2.174\n",
            "Batch: 100   D_Loss: 0.336   G_Loss: 2.213\n",
            "Batch: 125   D_Loss: 0.333   G_Loss: 2.303\n",
            "Batch: 150   D_Loss: 0.333   G_Loss: 2.295\n",
            "Batch: 175   D_Loss: 0.334   G_Loss: 2.257\n",
            "Batch: 200   D_Loss: 0.332   G_Loss: 2.321\n",
            "Batch: 225   D_Loss: 0.331   G_Loss: 2.292\n",
            "Batch: 250   D_Loss: 0.331   G_Loss: 2.291\n",
            "Batch: 275   D_Loss: 0.334   G_Loss: 2.308\n",
            "Batch: 300   D_Loss: 0.337   G_Loss: 2.249\n",
            "Batch: 325   D_Loss: 0.335   G_Loss: 2.265\n",
            "Batch: 350   D_Loss: 0.335   G_Loss: 2.261\n",
            "Batch: 375   D_Loss: 0.334   G_Loss: 2.261\n",
            "Batch: 400   D_Loss: 0.333   G_Loss: 2.325\n",
            "Batch: 425   D_Loss: 0.333   G_Loss: 2.303\n",
            "Batch: 450   D_Loss: 0.332   G_Loss: 2.288\n",
            "Batch: 475   D_Loss: 0.332   G_Loss: 2.312\n",
            "Batch: 500   D_Loss: 0.333   G_Loss: 2.312\n",
            "Batch: 525   D_Loss: 0.334   G_Loss: 2.286\n",
            "Batch: 550   D_Loss: 0.332   G_Loss: 2.242\n",
            "Batch: 575   D_Loss: 0.331   G_Loss: 2.319\n",
            "Batch: 600   D_Loss: 0.331   G_Loss: 2.311\n",
            "Batch: 625   D_Loss: 0.343   G_Loss: 2.226\n",
            "Batch: 650   D_Loss: 0.337   G_Loss: 2.196\n",
            "Batch: 675   D_Loss: 0.336   G_Loss: 2.265\n",
            "Batch: 700   D_Loss: 0.334   G_Loss: 2.295\n",
            "Batch: 725   D_Loss: 0.336   G_Loss: 2.262\n",
            "Batch: 750   D_Loss: 0.333   G_Loss: 2.296\n",
            "Batch: 775   D_Loss: 0.332   G_Loss: 2.329\n",
            "Batch: 800   D_Loss: 0.331   G_Loss: 2.317\n",
            "Batch: 825   D_Loss: 0.333   G_Loss: 2.293\n",
            "Batch: 850   D_Loss: 0.331   G_Loss: 2.269\n",
            "Batch: 875   D_Loss: 0.331   G_Loss: 2.306\n",
            "Batch: 900   D_Loss: 0.333   G_Loss: 2.266\n",
            "Batch: 925   D_Loss: 0.332   G_Loss: 2.301\n",
            "Batch: 950   D_Loss: 0.336   G_Loss: 2.280\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.301\n",
            "Batch: 1000   D_Loss: 0.332   G_Loss: 2.304\n",
            "Batch: 1025   D_Loss: 0.331   G_Loss: 2.283\n",
            "Batch: 1050   D_Loss: 0.332   G_Loss: 2.324\n",
            "Batch: 1075   D_Loss: 0.331   G_Loss: 2.341\n",
            "Batch: 1100   D_Loss: 0.332   G_Loss: 2.302\n",
            "Batch: 1125   D_Loss: 0.332   G_Loss: 2.321\n",
            "Batch: 1150   D_Loss: 0.334   G_Loss: 2.266\n",
            "Batch: 1175   D_Loss: 0.331   G_Loss: 2.297\n",
            "Batch: 1200   D_Loss: 0.331   G_Loss: 2.325\n",
            "Batch: 1225   D_Loss: 0.332   G_Loss: 2.307\n",
            "Batch: 1250   D_Loss: 0.332   G_Loss: 2.274\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.302\n",
            "Batch: 1300   D_Loss: 0.334   G_Loss: 2.271\n",
            "Batch: 1325   D_Loss: 0.332   G_Loss: 2.300\n",
            "Batch: 1350   D_Loss: 0.332   G_Loss: 2.294\n",
            "Batch: 1375   D_Loss: 0.331   G_Loss: 2.297\n",
            "Batch: 1400   D_Loss: 0.333   G_Loss: 2.279\n",
            "Batch: 1425   D_Loss: 0.332   G_Loss: 2.292\n",
            "Batch: 1450   D_Loss: 0.331   G_Loss: 2.298\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.290\n",
            "Batch: 1500   D_Loss: 0.332   G_Loss: 2.267\n",
            "Batch: 1525   D_Loss: 0.332   G_Loss: 2.278\n",
            "Batch: 1550   D_Loss: 0.331   G_Loss: 2.345\n",
            "Batch: 1575   D_Loss: 0.331   G_Loss: 2.307\n",
            "Batch: 1600   D_Loss: 0.332   G_Loss: 2.289\n",
            "Batch: 1625   D_Loss: 0.331   G_Loss: 2.312\n",
            "Batch: 1650   D_Loss: 0.331   G_Loss: 2.285\n",
            "Batch: 1675   D_Loss: 0.332   G_Loss: 2.291\n",
            "Batch: 1700   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 1725   D_Loss: 0.331   G_Loss: 2.298\n",
            "Batch: 1750   D_Loss: 0.331   G_Loss: 2.310\n",
            "Batch: 1775   D_Loss: 0.334   G_Loss: 2.273\n",
            "Batch: 1800   D_Loss: 0.336   G_Loss: 2.262\n",
            "Batch: 1825   D_Loss: 0.333   G_Loss: 2.249\n",
            "Batch: 1850   D_Loss: 0.332   G_Loss: 2.320\n",
            "Batch: 1875   D_Loss: 0.332   G_Loss: 2.295\n",
            "Epoch 7\n",
            "Batch: 25   D_Loss: 0.332   G_Loss: 2.312\n",
            "Batch: 50   D_Loss: 0.332   G_Loss: 2.322\n",
            "Batch: 75   D_Loss: 0.330   G_Loss: 2.280\n",
            "Batch: 100   D_Loss: 0.331   G_Loss: 2.320\n",
            "Batch: 125   D_Loss: 0.331   G_Loss: 2.319\n",
            "Batch: 150   D_Loss: 0.331   G_Loss: 2.286\n",
            "Batch: 175   D_Loss: 0.331   G_Loss: 2.313\n",
            "Batch: 200   D_Loss: 0.331   G_Loss: 2.302\n",
            "Batch: 225   D_Loss: 0.331   G_Loss: 2.317\n",
            "Batch: 250   D_Loss: 0.333   G_Loss: 2.286\n",
            "Batch: 275   D_Loss: 0.330   G_Loss: 2.329\n",
            "Batch: 300   D_Loss: 0.331   G_Loss: 2.273\n",
            "Batch: 325   D_Loss: 0.333   G_Loss: 2.306\n",
            "Batch: 350   D_Loss: 0.333   G_Loss: 2.276\n",
            "Batch: 375   D_Loss: 0.333   G_Loss: 2.294\n",
            "Batch: 400   D_Loss: 0.331   G_Loss: 2.305\n",
            "Batch: 425   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 450   D_Loss: 0.331   G_Loss: 2.295\n",
            "Batch: 475   D_Loss: 0.331   G_Loss: 2.293\n",
            "Batch: 500   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 525   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 550   D_Loss: 0.332   G_Loss: 2.291\n",
            "Batch: 575   D_Loss: 0.332   G_Loss: 2.258\n",
            "Batch: 600   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 625   D_Loss: 0.332   G_Loss: 2.258\n",
            "Batch: 650   D_Loss: 0.331   G_Loss: 2.295\n",
            "Batch: 675   D_Loss: 0.330   G_Loss: 2.291\n",
            "Batch: 700   D_Loss: 0.332   G_Loss: 2.303\n",
            "Batch: 725   D_Loss: 0.332   G_Loss: 2.250\n",
            "Batch: 750   D_Loss: 0.331   G_Loss: 2.300\n",
            "Batch: 775   D_Loss: 0.331   G_Loss: 2.296\n",
            "Batch: 800   D_Loss: 0.331   G_Loss: 2.309\n",
            "Batch: 825   D_Loss: 0.331   G_Loss: 2.318\n",
            "Batch: 850   D_Loss: 0.331   G_Loss: 2.331\n",
            "Batch: 875   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 900   D_Loss: 0.331   G_Loss: 2.315\n",
            "Batch: 925   D_Loss: 0.332   G_Loss: 2.309\n",
            "Batch: 950   D_Loss: 0.331   G_Loss: 2.328\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.265\n",
            "Batch: 1000   D_Loss: 0.331   G_Loss: 2.279\n",
            "Batch: 1025   D_Loss: 0.330   G_Loss: 2.288\n",
            "Batch: 1050   D_Loss: 0.330   G_Loss: 2.270\n",
            "Batch: 1075   D_Loss: 0.330   G_Loss: 2.321\n",
            "Batch: 1100   D_Loss: 0.331   G_Loss: 2.278\n",
            "Batch: 1125   D_Loss: 0.331   G_Loss: 2.287\n",
            "Batch: 1150   D_Loss: 0.331   G_Loss: 2.295\n",
            "Batch: 1175   D_Loss: 0.332   G_Loss: 2.259\n",
            "Batch: 1200   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 1225   D_Loss: 0.332   G_Loss: 2.254\n",
            "Batch: 1250   D_Loss: 0.330   G_Loss: 2.305\n",
            "Batch: 1275   D_Loss: 0.330   G_Loss: 2.290\n",
            "Batch: 1300   D_Loss: 0.330   G_Loss: 2.313\n",
            "Batch: 1325   D_Loss: 0.332   G_Loss: 2.268\n",
            "Batch: 1350   D_Loss: 0.335   G_Loss: 2.248\n",
            "Batch: 1375   D_Loss: 0.334   G_Loss: 2.235\n",
            "Batch: 1400   D_Loss: 0.334   G_Loss: 2.264\n",
            "Batch: 1425   D_Loss: 0.333   G_Loss: 2.307\n",
            "Batch: 1450   D_Loss: 0.331   G_Loss: 2.316\n",
            "Batch: 1475   D_Loss: 0.330   G_Loss: 2.294\n",
            "Batch: 1500   D_Loss: 0.331   G_Loss: 2.314\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.273\n",
            "Batch: 1550   D_Loss: 0.330   G_Loss: 2.296\n",
            "Batch: 1575   D_Loss: 0.330   G_Loss: 2.324\n",
            "Batch: 1600   D_Loss: 0.330   G_Loss: 2.298\n",
            "Batch: 1625   D_Loss: 0.334   G_Loss: 2.299\n",
            "Batch: 1650   D_Loss: 0.334   G_Loss: 2.272\n",
            "Batch: 1675   D_Loss: 0.343   G_Loss: 2.188\n",
            "Batch: 1700   D_Loss: 0.340   G_Loss: 2.225\n",
            "Batch: 1725   D_Loss: 0.335   G_Loss: 2.230\n",
            "Batch: 1750   D_Loss: 0.334   G_Loss: 2.301\n",
            "Batch: 1775   D_Loss: 0.332   G_Loss: 2.286\n",
            "Batch: 1800   D_Loss: 0.331   G_Loss: 2.287\n",
            "Batch: 1825   D_Loss: 0.331   G_Loss: 2.309\n",
            "Batch: 1850   D_Loss: 0.331   G_Loss: 2.297\n",
            "Batch: 1875   D_Loss: 0.330   G_Loss: 2.291\n",
            "Epoch 8\n",
            "Batch: 25   D_Loss: 0.334   G_Loss: 2.281\n",
            "Batch: 50   D_Loss: 0.331   G_Loss: 2.282\n",
            "Batch: 75   D_Loss: 0.335   G_Loss: 2.226\n",
            "Batch: 100   D_Loss: 0.332   G_Loss: 2.244\n",
            "Batch: 125   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 150   D_Loss: 0.332   G_Loss: 2.304\n",
            "Batch: 175   D_Loss: 0.333   G_Loss: 2.274\n",
            "Batch: 200   D_Loss: 0.330   G_Loss: 2.311\n",
            "Batch: 225   D_Loss: 0.332   G_Loss: 2.273\n",
            "Batch: 250   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 275   D_Loss: 0.331   G_Loss: 2.325\n",
            "Batch: 300   D_Loss: 0.330   G_Loss: 2.299\n",
            "Batch: 325   D_Loss: 0.330   G_Loss: 2.300\n",
            "Batch: 350   D_Loss: 0.330   G_Loss: 2.310\n",
            "Batch: 375   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 400   D_Loss: 0.330   G_Loss: 2.290\n",
            "Batch: 425   D_Loss: 0.330   G_Loss: 2.325\n",
            "Batch: 450   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 475   D_Loss: 0.332   G_Loss: 2.310\n",
            "Batch: 500   D_Loss: 0.331   G_Loss: 2.312\n",
            "Batch: 525   D_Loss: 0.332   G_Loss: 2.303\n",
            "Batch: 550   D_Loss: 0.331   G_Loss: 2.308\n",
            "Batch: 575   D_Loss: 0.332   G_Loss: 2.274\n",
            "Batch: 600   D_Loss: 0.333   G_Loss: 2.269\n",
            "Batch: 625   D_Loss: 0.330   G_Loss: 2.301\n",
            "Batch: 650   D_Loss: 0.331   G_Loss: 2.290\n",
            "Batch: 675   D_Loss: 0.331   G_Loss: 2.298\n",
            "Batch: 700   D_Loss: 0.332   G_Loss: 2.277\n",
            "Batch: 725   D_Loss: 0.331   G_Loss: 2.288\n",
            "Batch: 750   D_Loss: 0.330   G_Loss: 2.341\n",
            "Batch: 775   D_Loss: 0.331   G_Loss: 2.294\n",
            "Batch: 800   D_Loss: 0.330   G_Loss: 2.275\n",
            "Batch: 825   D_Loss: 0.330   G_Loss: 2.323\n",
            "Batch: 850   D_Loss: 0.331   G_Loss: 2.293\n",
            "Batch: 875   D_Loss: 0.331   G_Loss: 2.305\n",
            "Batch: 900   D_Loss: 0.330   G_Loss: 2.285\n",
            "Batch: 925   D_Loss: 0.330   G_Loss: 2.332\n",
            "Batch: 950   D_Loss: 0.331   G_Loss: 2.277\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.327\n",
            "Batch: 1000   D_Loss: 0.331   G_Loss: 2.300\n",
            "Batch: 1025   D_Loss: 0.330   G_Loss: 2.335\n",
            "Batch: 1050   D_Loss: 0.329   G_Loss: 2.338\n",
            "Batch: 1075   D_Loss: 0.331   G_Loss: 2.257\n",
            "Batch: 1100   D_Loss: 0.331   G_Loss: 2.293\n",
            "Batch: 1125   D_Loss: 0.332   G_Loss: 2.308\n",
            "Batch: 1150   D_Loss: 0.331   G_Loss: 2.310\n",
            "Batch: 1175   D_Loss: 0.332   G_Loss: 2.253\n",
            "Batch: 1200   D_Loss: 0.333   G_Loss: 2.267\n",
            "Batch: 1225   D_Loss: 0.332   G_Loss: 2.276\n",
            "Batch: 1250   D_Loss: 0.330   G_Loss: 2.275\n",
            "Batch: 1275   D_Loss: 0.330   G_Loss: 2.310\n",
            "Batch: 1300   D_Loss: 0.330   G_Loss: 2.286\n",
            "Batch: 1325   D_Loss: 0.330   G_Loss: 2.315\n",
            "Batch: 1350   D_Loss: 0.331   G_Loss: 2.315\n",
            "Batch: 1375   D_Loss: 0.331   G_Loss: 2.279\n",
            "Batch: 1400   D_Loss: 0.330   G_Loss: 2.303\n",
            "Batch: 1425   D_Loss: 0.332   G_Loss: 2.274\n",
            "Batch: 1450   D_Loss: 0.332   G_Loss: 2.314\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.326\n",
            "Batch: 1500   D_Loss: 0.331   G_Loss: 2.296\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.309\n",
            "Batch: 1550   D_Loss: 0.332   G_Loss: 2.283\n",
            "Batch: 1575   D_Loss: 0.331   G_Loss: 2.314\n",
            "Batch: 1600   D_Loss: 0.339   G_Loss: 2.196\n",
            "Batch: 1625   D_Loss: 0.352   G_Loss: 2.424\n",
            "Batch: 1650   D_Loss: 0.387   G_Loss: 2.486\n",
            "Batch: 1675   D_Loss: 0.381   G_Loss: 2.521\n",
            "Batch: 1700   D_Loss: 0.383   G_Loss: 2.414\n",
            "Batch: 1725   D_Loss: 0.368   G_Loss: 2.282\n",
            "Batch: 1750   D_Loss: 0.365   G_Loss: 2.309\n",
            "Batch: 1775   D_Loss: 0.352   G_Loss: 2.209\n",
            "Batch: 1800   D_Loss: 0.343   G_Loss: 2.234\n",
            "Batch: 1825   D_Loss: 0.339   G_Loss: 2.179\n",
            "Batch: 1850   D_Loss: 0.340   G_Loss: 2.159\n",
            "Batch: 1875   D_Loss: 0.337   G_Loss: 2.204\n",
            "Epoch 9\n",
            "Batch: 25   D_Loss: 0.335   G_Loss: 2.233\n",
            "Batch: 50   D_Loss: 0.335   G_Loss: 2.161\n",
            "Batch: 75   D_Loss: 0.335   G_Loss: 2.154\n",
            "Batch: 100   D_Loss: 0.335   G_Loss: 2.187\n",
            "Batch: 125   D_Loss: 0.338   G_Loss: 2.234\n",
            "Batch: 150   D_Loss: 0.334   G_Loss: 2.219\n",
            "Batch: 175   D_Loss: 0.337   G_Loss: 2.220\n",
            "Batch: 200   D_Loss: 0.341   G_Loss: 2.176\n",
            "Batch: 225   D_Loss: 0.339   G_Loss: 2.177\n",
            "Batch: 250   D_Loss: 0.335   G_Loss: 2.174\n",
            "Batch: 275   D_Loss: 0.339   G_Loss: 2.186\n",
            "Batch: 300   D_Loss: 0.338   G_Loss: 2.203\n",
            "Batch: 325   D_Loss: 0.339   G_Loss: 2.256\n",
            "Batch: 350   D_Loss: 0.338   G_Loss: 2.178\n",
            "Batch: 375   D_Loss: 0.337   G_Loss: 2.288\n",
            "Batch: 400   D_Loss: 0.337   G_Loss: 2.284\n",
            "Batch: 425   D_Loss: 0.333   G_Loss: 2.271\n",
            "Batch: 450   D_Loss: 0.332   G_Loss: 2.250\n",
            "Batch: 475   D_Loss: 0.334   G_Loss: 2.271\n",
            "Batch: 500   D_Loss: 0.337   G_Loss: 2.357\n",
            "Batch: 525   D_Loss: 0.332   G_Loss: 2.362\n",
            "Batch: 550   D_Loss: 0.334   G_Loss: 2.316\n",
            "Batch: 575   D_Loss: 0.334   G_Loss: 2.310\n",
            "Batch: 600   D_Loss: 0.336   G_Loss: 2.314\n",
            "Batch: 625   D_Loss: 0.337   G_Loss: 2.360\n",
            "Batch: 650   D_Loss: 0.337   G_Loss: 2.297\n",
            "Batch: 675   D_Loss: 0.333   G_Loss: 2.275\n",
            "Batch: 700   D_Loss: 0.334   G_Loss: 2.284\n",
            "Batch: 725   D_Loss: 0.334   G_Loss: 2.269\n",
            "Batch: 750   D_Loss: 0.332   G_Loss: 2.316\n",
            "Batch: 775   D_Loss: 0.330   G_Loss: 2.323\n",
            "Batch: 800   D_Loss: 0.336   G_Loss: 2.238\n",
            "Batch: 825   D_Loss: 0.343   G_Loss: 2.278\n",
            "Batch: 850   D_Loss: 0.339   G_Loss: 2.285\n",
            "Batch: 875   D_Loss: 0.336   G_Loss: 2.376\n",
            "Batch: 900   D_Loss: 0.334   G_Loss: 2.282\n",
            "Batch: 925   D_Loss: 0.337   G_Loss: 2.357\n",
            "Batch: 950   D_Loss: 0.340   G_Loss: 2.353\n",
            "Batch: 975   D_Loss: 0.339   G_Loss: 2.370\n",
            "Batch: 1000   D_Loss: 0.341   G_Loss: 2.412\n",
            "Batch: 1025   D_Loss: 0.340   G_Loss: 2.378\n",
            "Batch: 1050   D_Loss: 0.336   G_Loss: 2.297\n",
            "Batch: 1075   D_Loss: 0.339   G_Loss: 2.290\n",
            "Batch: 1100   D_Loss: 0.334   G_Loss: 2.289\n",
            "Batch: 1125   D_Loss: 0.337   G_Loss: 2.318\n",
            "Batch: 1150   D_Loss: 0.333   G_Loss: 2.306\n",
            "Batch: 1175   D_Loss: 0.336   G_Loss: 2.252\n",
            "Batch: 1200   D_Loss: 0.335   G_Loss: 2.401\n",
            "Batch: 1225   D_Loss: 0.334   G_Loss: 2.346\n",
            "Batch: 1250   D_Loss: 0.333   G_Loss: 2.321\n",
            "Batch: 1275   D_Loss: 0.334   G_Loss: 2.328\n",
            "Batch: 1300   D_Loss: 0.335   G_Loss: 2.346\n",
            "Batch: 1325   D_Loss: 0.339   G_Loss: 2.263\n",
            "Batch: 1350   D_Loss: 0.334   G_Loss: 2.282\n",
            "Batch: 1375   D_Loss: 0.336   G_Loss: 2.327\n",
            "Batch: 1400   D_Loss: 0.336   G_Loss: 2.302\n",
            "Batch: 1425   D_Loss: 0.333   G_Loss: 2.345\n",
            "Batch: 1450   D_Loss: 0.333   G_Loss: 2.334\n",
            "Batch: 1475   D_Loss: 0.334   G_Loss: 2.321\n",
            "Batch: 1500   D_Loss: 0.332   G_Loss: 2.300\n",
            "Batch: 1525   D_Loss: 0.333   G_Loss: 2.288\n",
            "Batch: 1550   D_Loss: 0.334   G_Loss: 2.316\n",
            "Batch: 1575   D_Loss: 0.335   G_Loss: 2.308\n",
            "Batch: 1600   D_Loss: 0.336   G_Loss: 2.312\n",
            "Batch: 1625   D_Loss: 0.337   G_Loss: 2.308\n",
            "Batch: 1650   D_Loss: 0.337   G_Loss: 2.316\n",
            "Batch: 1675   D_Loss: 0.338   G_Loss: 2.307\n",
            "Batch: 1700   D_Loss: 0.343   G_Loss: 2.240\n",
            "Batch: 1725   D_Loss: 0.336   G_Loss: 2.277\n",
            "Batch: 1750   D_Loss: 0.340   G_Loss: 2.266\n",
            "Batch: 1775   D_Loss: 0.339   G_Loss: 2.281\n",
            "Batch: 1800   D_Loss: 0.339   G_Loss: 2.319\n",
            "Batch: 1825   D_Loss: 0.337   G_Loss: 2.242\n",
            "Batch: 1850   D_Loss: 0.336   G_Loss: 2.299\n",
            "Batch: 1875   D_Loss: 0.336   G_Loss: 2.296\n",
            "Epoch 10\n",
            "Batch: 25   D_Loss: 0.336   G_Loss: 2.289\n",
            "Batch: 50   D_Loss: 0.337   G_Loss: 2.275\n",
            "Batch: 75   D_Loss: 0.336   G_Loss: 2.241\n",
            "Batch: 100   D_Loss: 0.339   G_Loss: 2.288\n",
            "Batch: 125   D_Loss: 0.343   G_Loss: 2.234\n",
            "Batch: 150   D_Loss: 0.338   G_Loss: 2.222\n",
            "Batch: 175   D_Loss: 0.335   G_Loss: 2.302\n",
            "Batch: 200   D_Loss: 0.334   G_Loss: 2.235\n",
            "Batch: 225   D_Loss: 0.337   G_Loss: 2.144\n",
            "Batch: 250   D_Loss: 0.337   G_Loss: 2.288\n",
            "Batch: 275   D_Loss: 0.338   G_Loss: 2.323\n",
            "Batch: 300   D_Loss: 0.343   G_Loss: 2.156\n",
            "Batch: 325   D_Loss: 0.336   G_Loss: 2.131\n",
            "Batch: 350   D_Loss: 0.336   G_Loss: 2.228\n",
            "Batch: 375   D_Loss: 0.334   G_Loss: 2.181\n",
            "Batch: 400   D_Loss: 0.336   G_Loss: 2.258\n",
            "Batch: 425   D_Loss: 0.335   G_Loss: 2.205\n",
            "Batch: 450   D_Loss: 0.333   G_Loss: 2.239\n",
            "Batch: 475   D_Loss: 0.335   G_Loss: 2.216\n",
            "Batch: 500   D_Loss: 0.334   G_Loss: 2.297\n",
            "Batch: 525   D_Loss: 0.335   G_Loss: 2.281\n",
            "Batch: 550   D_Loss: 0.335   G_Loss: 2.216\n",
            "Batch: 575   D_Loss: 0.334   G_Loss: 2.273\n",
            "Batch: 600   D_Loss: 0.332   G_Loss: 2.278\n",
            "Batch: 625   D_Loss: 0.332   G_Loss: 2.296\n",
            "Batch: 650   D_Loss: 0.334   G_Loss: 2.352\n",
            "Batch: 675   D_Loss: 0.334   G_Loss: 2.313\n",
            "Batch: 700   D_Loss: 0.335   G_Loss: 2.303\n",
            "Batch: 725   D_Loss: 0.334   G_Loss: 2.384\n",
            "Batch: 750   D_Loss: 0.334   G_Loss: 2.225\n",
            "Batch: 775   D_Loss: 0.332   G_Loss: 2.318\n",
            "Batch: 800   D_Loss: 0.332   G_Loss: 2.297\n",
            "Batch: 825   D_Loss: 0.332   G_Loss: 2.222\n",
            "Batch: 850   D_Loss: 0.332   G_Loss: 2.276\n",
            "Batch: 875   D_Loss: 0.335   G_Loss: 2.251\n",
            "Batch: 900   D_Loss: 0.334   G_Loss: 2.247\n",
            "Batch: 925   D_Loss: 0.332   G_Loss: 2.248\n",
            "Batch: 950   D_Loss: 0.332   G_Loss: 2.287\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.301\n",
            "Batch: 1000   D_Loss: 0.332   G_Loss: 2.298\n",
            "Batch: 1025   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 1050   D_Loss: 0.332   G_Loss: 2.376\n",
            "Batch: 1075   D_Loss: 0.332   G_Loss: 2.318\n",
            "Batch: 1100   D_Loss: 0.332   G_Loss: 2.307\n",
            "Batch: 1125   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 1150   D_Loss: 0.330   G_Loss: 2.313\n",
            "Batch: 1175   D_Loss: 0.330   G_Loss: 2.267\n",
            "Batch: 1200   D_Loss: 0.333   G_Loss: 2.387\n",
            "Batch: 1225   D_Loss: 0.334   G_Loss: 2.358\n",
            "Batch: 1250   D_Loss: 0.333   G_Loss: 2.300\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.277\n",
            "Batch: 1300   D_Loss: 0.333   G_Loss: 2.366\n",
            "Batch: 1325   D_Loss: 0.332   G_Loss: 2.312\n",
            "Batch: 1350   D_Loss: 0.334   G_Loss: 2.310\n",
            "Batch: 1375   D_Loss: 0.331   G_Loss: 2.314\n",
            "Batch: 1400   D_Loss: 0.330   G_Loss: 2.316\n",
            "Batch: 1425   D_Loss: 0.330   G_Loss: 2.288\n",
            "Batch: 1450   D_Loss: 0.334   G_Loss: 2.311\n",
            "Batch: 1475   D_Loss: 0.331   G_Loss: 2.322\n",
            "Batch: 1500   D_Loss: 0.331   G_Loss: 2.331\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.352\n",
            "Batch: 1550   D_Loss: 0.331   G_Loss: 2.292\n",
            "Batch: 1575   D_Loss: 0.330   G_Loss: 2.315\n",
            "Batch: 1600   D_Loss: 0.330   G_Loss: 2.300\n",
            "Batch: 1625   D_Loss: 0.330   G_Loss: 2.357\n",
            "Batch: 1650   D_Loss: 0.331   G_Loss: 2.308\n",
            "Batch: 1675   D_Loss: 0.330   G_Loss: 2.311\n",
            "Batch: 1700   D_Loss: 0.332   G_Loss: 2.326\n",
            "Batch: 1725   D_Loss: 0.333   G_Loss: 2.327\n",
            "Batch: 1750   D_Loss: 0.333   G_Loss: 2.328\n",
            "Batch: 1775   D_Loss: 0.332   G_Loss: 2.328\n",
            "Batch: 1800   D_Loss: 0.334   G_Loss: 2.318\n",
            "Batch: 1825   D_Loss: 0.332   G_Loss: 2.318\n",
            "Batch: 1850   D_Loss: 0.331   G_Loss: 2.324\n",
            "Batch: 1875   D_Loss: 0.332   G_Loss: 2.308\n",
            "Epoch 11\n",
            "Batch: 25   D_Loss: 0.332   G_Loss: 2.363\n",
            "Batch: 50   D_Loss: 0.332   G_Loss: 2.295\n",
            "Batch: 75   D_Loss: 0.332   G_Loss: 2.297\n",
            "Batch: 100   D_Loss: 0.331   G_Loss: 2.304\n",
            "Batch: 125   D_Loss: 0.333   G_Loss: 2.329\n",
            "Batch: 150   D_Loss: 0.334   G_Loss: 2.303\n",
            "Batch: 175   D_Loss: 0.332   G_Loss: 2.375\n",
            "Batch: 200   D_Loss: 0.331   G_Loss: 2.313\n",
            "Batch: 225   D_Loss: 0.334   G_Loss: 2.302\n",
            "Batch: 250   D_Loss: 0.335   G_Loss: 2.249\n",
            "Batch: 275   D_Loss: 0.335   G_Loss: 2.297\n",
            "Batch: 300   D_Loss: 0.335   G_Loss: 2.255\n",
            "Batch: 325   D_Loss: 0.356   G_Loss: 2.267\n",
            "Batch: 350   D_Loss: 0.339   G_Loss: 2.280\n",
            "Batch: 375   D_Loss: 0.342   G_Loss: 2.078\n",
            "Batch: 400   D_Loss: 0.344   G_Loss: 2.056\n",
            "Batch: 425   D_Loss: 0.341   G_Loss: 2.092\n",
            "Batch: 450   D_Loss: 0.339   G_Loss: 2.114\n",
            "Batch: 475   D_Loss: 0.338   G_Loss: 2.185\n",
            "Batch: 500   D_Loss: 0.334   G_Loss: 2.193\n",
            "Batch: 525   D_Loss: 0.334   G_Loss: 2.226\n",
            "Batch: 550   D_Loss: 0.334   G_Loss: 2.180\n",
            "Batch: 575   D_Loss: 0.331   G_Loss: 2.211\n",
            "Batch: 600   D_Loss: 0.331   G_Loss: 2.253\n",
            "Batch: 625   D_Loss: 0.334   G_Loss: 2.199\n",
            "Batch: 650   D_Loss: 0.333   G_Loss: 2.245\n",
            "Batch: 675   D_Loss: 0.334   G_Loss: 2.241\n",
            "Batch: 700   D_Loss: 0.336   G_Loss: 2.222\n",
            "Batch: 725   D_Loss: 0.333   G_Loss: 2.198\n",
            "Batch: 750   D_Loss: 0.336   G_Loss: 2.275\n",
            "Batch: 775   D_Loss: 0.333   G_Loss: 2.183\n",
            "Batch: 800   D_Loss: 0.331   G_Loss: 2.258\n",
            "Batch: 825   D_Loss: 0.333   G_Loss: 2.317\n",
            "Batch: 850   D_Loss: 0.336   G_Loss: 2.204\n",
            "Batch: 875   D_Loss: 0.333   G_Loss: 2.255\n",
            "Batch: 900   D_Loss: 0.332   G_Loss: 2.243\n",
            "Batch: 925   D_Loss: 0.332   G_Loss: 2.270\n",
            "Batch: 950   D_Loss: 0.333   G_Loss: 2.281\n",
            "Batch: 975   D_Loss: 0.331   G_Loss: 2.269\n",
            "Batch: 1000   D_Loss: 0.336   G_Loss: 2.219\n",
            "Batch: 1025   D_Loss: 0.334   G_Loss: 2.292\n",
            "Batch: 1050   D_Loss: 0.332   G_Loss: 2.258\n",
            "Batch: 1075   D_Loss: 0.333   G_Loss: 2.240\n",
            "Batch: 1100   D_Loss: 0.333   G_Loss: 2.255\n",
            "Batch: 1125   D_Loss: 0.336   G_Loss: 2.184\n",
            "Batch: 1150   D_Loss: 0.334   G_Loss: 2.198\n",
            "Batch: 1175   D_Loss: 0.338   G_Loss: 2.252\n",
            "Batch: 1200   D_Loss: 0.334   G_Loss: 2.193\n",
            "Batch: 1225   D_Loss: 0.332   G_Loss: 2.238\n",
            "Batch: 1250   D_Loss: 0.333   G_Loss: 2.284\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.252\n",
            "Batch: 1300   D_Loss: 0.332   G_Loss: 2.303\n",
            "Batch: 1325   D_Loss: 0.332   G_Loss: 2.297\n",
            "Batch: 1350   D_Loss: 0.331   G_Loss: 2.310\n",
            "Batch: 1375   D_Loss: 0.332   G_Loss: 2.311\n",
            "Batch: 1400   D_Loss: 0.331   G_Loss: 2.254\n",
            "Batch: 1425   D_Loss: 0.333   G_Loss: 2.252\n",
            "Batch: 1450   D_Loss: 0.331   G_Loss: 2.277\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.328\n",
            "Batch: 1500   D_Loss: 0.330   G_Loss: 2.273\n",
            "Batch: 1525   D_Loss: 0.332   G_Loss: 2.296\n",
            "Batch: 1550   D_Loss: 0.330   G_Loss: 2.247\n",
            "Batch: 1575   D_Loss: 0.332   G_Loss: 2.275\n",
            "Batch: 1600   D_Loss: 0.331   G_Loss: 2.280\n",
            "Batch: 1625   D_Loss: 0.335   G_Loss: 2.287\n",
            "Batch: 1650   D_Loss: 0.332   G_Loss: 2.259\n",
            "Batch: 1675   D_Loss: 0.333   G_Loss: 2.318\n",
            "Batch: 1700   D_Loss: 0.333   G_Loss: 2.254\n",
            "Batch: 1725   D_Loss: 0.333   G_Loss: 2.236\n",
            "Batch: 1750   D_Loss: 0.332   G_Loss: 2.270\n",
            "Batch: 1775   D_Loss: 0.333   G_Loss: 2.280\n",
            "Batch: 1800   D_Loss: 0.333   G_Loss: 2.317\n",
            "Batch: 1825   D_Loss: 0.333   G_Loss: 2.255\n",
            "Batch: 1850   D_Loss: 0.332   G_Loss: 2.307\n",
            "Batch: 1875   D_Loss: 0.331   G_Loss: 2.268\n",
            "Epoch 12\n",
            "Batch: 25   D_Loss: 0.331   G_Loss: 2.312\n",
            "Batch: 50   D_Loss: 0.330   G_Loss: 2.294\n",
            "Batch: 75   D_Loss: 0.332   G_Loss: 2.302\n",
            "Batch: 100   D_Loss: 0.332   G_Loss: 2.296\n",
            "Batch: 125   D_Loss: 0.332   G_Loss: 2.268\n",
            "Batch: 150   D_Loss: 0.330   G_Loss: 2.309\n",
            "Batch: 175   D_Loss: 0.331   G_Loss: 2.287\n",
            "Batch: 200   D_Loss: 0.330   G_Loss: 2.304\n",
            "Batch: 225   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 250   D_Loss: 0.330   G_Loss: 2.320\n",
            "Batch: 275   D_Loss: 0.332   G_Loss: 2.269\n",
            "Batch: 300   D_Loss: 0.332   G_Loss: 2.303\n",
            "Batch: 325   D_Loss: 0.334   G_Loss: 2.256\n",
            "Batch: 350   D_Loss: 0.334   G_Loss: 2.291\n",
            "Batch: 375   D_Loss: 0.335   G_Loss: 2.296\n",
            "Batch: 400   D_Loss: 0.335   G_Loss: 2.321\n",
            "Batch: 425   D_Loss: 0.333   G_Loss: 2.317\n",
            "Batch: 450   D_Loss: 0.334   G_Loss: 2.289\n",
            "Batch: 475   D_Loss: 0.336   G_Loss: 2.251\n",
            "Batch: 500   D_Loss: 0.339   G_Loss: 2.278\n",
            "Batch: 525   D_Loss: 0.340   G_Loss: 2.380\n",
            "Batch: 550   D_Loss: 0.340   G_Loss: 2.380\n",
            "Batch: 575   D_Loss: 0.337   G_Loss: 2.368\n",
            "Batch: 600   D_Loss: 0.334   G_Loss: 2.313\n",
            "Batch: 625   D_Loss: 0.333   G_Loss: 2.272\n",
            "Batch: 650   D_Loss: 0.333   G_Loss: 2.295\n",
            "Batch: 675   D_Loss: 0.333   G_Loss: 2.252\n",
            "Batch: 700   D_Loss: 0.333   G_Loss: 2.270\n",
            "Batch: 725   D_Loss: 0.332   G_Loss: 2.254\n",
            "Batch: 750   D_Loss: 0.334   G_Loss: 2.280\n",
            "Batch: 775   D_Loss: 0.332   G_Loss: 2.293\n",
            "Batch: 800   D_Loss: 0.333   G_Loss: 2.285\n",
            "Batch: 825   D_Loss: 0.334   G_Loss: 2.273\n",
            "Batch: 850   D_Loss: 0.332   G_Loss: 2.263\n",
            "Batch: 875   D_Loss: 0.334   G_Loss: 2.259\n",
            "Batch: 900   D_Loss: 0.332   G_Loss: 2.253\n",
            "Batch: 925   D_Loss: 0.334   G_Loss: 2.214\n",
            "Batch: 950   D_Loss: 0.332   G_Loss: 2.266\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.267\n",
            "Batch: 1000   D_Loss: 0.332   G_Loss: 2.206\n",
            "Batch: 1025   D_Loss: 0.331   G_Loss: 2.281\n",
            "Batch: 1050   D_Loss: 0.333   G_Loss: 2.236\n",
            "Batch: 1075   D_Loss: 0.333   G_Loss: 2.247\n",
            "Batch: 1100   D_Loss: 0.335   G_Loss: 2.222\n",
            "Batch: 1125   D_Loss: 0.336   G_Loss: 2.188\n",
            "Batch: 1150   D_Loss: 0.333   G_Loss: 2.239\n",
            "Batch: 1175   D_Loss: 0.334   G_Loss: 2.183\n",
            "Batch: 1200   D_Loss: 0.334   G_Loss: 2.275\n",
            "Batch: 1225   D_Loss: 0.332   G_Loss: 2.254\n",
            "Batch: 1250   D_Loss: 0.332   G_Loss: 2.253\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.282\n",
            "Batch: 1300   D_Loss: 0.332   G_Loss: 2.266\n",
            "Batch: 1325   D_Loss: 0.331   G_Loss: 2.272\n",
            "Batch: 1350   D_Loss: 0.332   G_Loss: 2.249\n",
            "Batch: 1375   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 1400   D_Loss: 0.331   G_Loss: 2.292\n",
            "Batch: 1425   D_Loss: 0.329   G_Loss: 2.284\n",
            "Batch: 1450   D_Loss: 0.330   G_Loss: 2.270\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.271\n",
            "Batch: 1500   D_Loss: 0.331   G_Loss: 2.320\n",
            "Batch: 1525   D_Loss: 0.333   G_Loss: 2.292\n",
            "Batch: 1550   D_Loss: 0.332   G_Loss: 2.273\n",
            "Batch: 1575   D_Loss: 0.330   G_Loss: 2.304\n",
            "Batch: 1600   D_Loss: 0.331   G_Loss: 2.307\n",
            "Batch: 1625   D_Loss: 0.330   G_Loss: 2.281\n",
            "Batch: 1650   D_Loss: 0.330   G_Loss: 2.255\n",
            "Batch: 1675   D_Loss: 0.330   G_Loss: 2.273\n",
            "Batch: 1700   D_Loss: 0.331   G_Loss: 2.271\n",
            "Batch: 1725   D_Loss: 0.330   G_Loss: 2.297\n",
            "Batch: 1750   D_Loss: 0.332   G_Loss: 2.270\n",
            "Batch: 1775   D_Loss: 0.331   G_Loss: 2.296\n",
            "Batch: 1800   D_Loss: 0.332   G_Loss: 2.270\n",
            "Batch: 1825   D_Loss: 0.331   G_Loss: 2.272\n",
            "Batch: 1850   D_Loss: 0.330   G_Loss: 2.273\n",
            "Batch: 1875   D_Loss: 0.331   G_Loss: 2.284\n",
            "Epoch 13\n",
            "Batch: 25   D_Loss: 0.330   G_Loss: 2.241\n",
            "Batch: 50   D_Loss: 0.331   G_Loss: 2.313\n",
            "Batch: 75   D_Loss: 0.331   G_Loss: 2.302\n",
            "Batch: 100   D_Loss: 0.330   G_Loss: 2.282\n",
            "Batch: 125   D_Loss: 0.330   G_Loss: 2.248\n",
            "Batch: 150   D_Loss: 0.332   G_Loss: 2.290\n",
            "Batch: 175   D_Loss: 0.331   G_Loss: 2.294\n",
            "Batch: 200   D_Loss: 0.330   G_Loss: 2.290\n",
            "Batch: 225   D_Loss: 0.330   G_Loss: 2.242\n",
            "Batch: 250   D_Loss: 0.330   G_Loss: 2.318\n",
            "Batch: 275   D_Loss: 0.331   G_Loss: 2.293\n",
            "Batch: 300   D_Loss: 0.331   G_Loss: 2.287\n",
            "Batch: 325   D_Loss: 0.330   G_Loss: 2.297\n",
            "Batch: 350   D_Loss: 0.333   G_Loss: 2.296\n",
            "Batch: 375   D_Loss: 0.331   G_Loss: 2.242\n",
            "Batch: 400   D_Loss: 0.332   G_Loss: 2.297\n",
            "Batch: 425   D_Loss: 0.331   G_Loss: 2.299\n",
            "Batch: 450   D_Loss: 0.332   G_Loss: 2.268\n",
            "Batch: 475   D_Loss: 0.330   G_Loss: 2.260\n",
            "Batch: 500   D_Loss: 0.329   G_Loss: 2.286\n",
            "Batch: 525   D_Loss: 0.331   G_Loss: 2.275\n",
            "Batch: 550   D_Loss: 0.330   G_Loss: 2.270\n",
            "Batch: 575   D_Loss: 0.331   G_Loss: 2.304\n",
            "Batch: 600   D_Loss: 0.330   G_Loss: 2.298\n",
            "Batch: 625   D_Loss: 0.330   G_Loss: 2.274\n",
            "Batch: 650   D_Loss: 0.335   G_Loss: 2.265\n",
            "Batch: 675   D_Loss: 0.331   G_Loss: 2.222\n",
            "Batch: 700   D_Loss: 0.331   G_Loss: 2.269\n",
            "Batch: 725   D_Loss: 0.331   G_Loss: 2.305\n",
            "Batch: 750   D_Loss: 0.331   G_Loss: 2.283\n",
            "Batch: 775   D_Loss: 0.331   G_Loss: 2.300\n",
            "Batch: 800   D_Loss: 0.330   G_Loss: 2.265\n",
            "Batch: 825   D_Loss: 0.330   G_Loss: 2.288\n",
            "Batch: 850   D_Loss: 0.330   G_Loss: 2.304\n",
            "Batch: 875   D_Loss: 0.329   G_Loss: 2.274\n",
            "Batch: 900   D_Loss: 0.330   G_Loss: 2.272\n",
            "Batch: 925   D_Loss: 0.330   G_Loss: 2.309\n",
            "Batch: 950   D_Loss: 0.331   G_Loss: 2.318\n",
            "Batch: 975   D_Loss: 0.332   G_Loss: 2.259\n",
            "Batch: 1000   D_Loss: 0.331   G_Loss: 2.273\n",
            "Batch: 1025   D_Loss: 0.331   G_Loss: 2.266\n",
            "Batch: 1050   D_Loss: 0.340   G_Loss: 2.241\n",
            "Batch: 1075   D_Loss: 0.336   G_Loss: 2.240\n",
            "Batch: 1100   D_Loss: 0.334   G_Loss: 2.212\n",
            "Batch: 1125   D_Loss: 0.334   G_Loss: 2.302\n",
            "Batch: 1150   D_Loss: 0.332   G_Loss: 2.248\n",
            "Batch: 1175   D_Loss: 0.333   G_Loss: 2.260\n",
            "Batch: 1200   D_Loss: 0.332   G_Loss: 2.253\n",
            "Batch: 1225   D_Loss: 0.330   G_Loss: 2.270\n",
            "Batch: 1250   D_Loss: 0.330   G_Loss: 2.284\n",
            "Batch: 1275   D_Loss: 0.332   G_Loss: 2.279\n",
            "Batch: 1300   D_Loss: 0.330   G_Loss: 2.287\n",
            "Batch: 1325   D_Loss: 0.331   G_Loss: 2.283\n",
            "Batch: 1350   D_Loss: 0.331   G_Loss: 2.278\n",
            "Batch: 1375   D_Loss: 0.330   G_Loss: 2.230\n",
            "Batch: 1400   D_Loss: 0.331   G_Loss: 2.271\n",
            "Batch: 1425   D_Loss: 0.331   G_Loss: 2.206\n",
            "Batch: 1450   D_Loss: 0.332   G_Loss: 2.127\n",
            "Batch: 1475   D_Loss: 0.333   G_Loss: 2.210\n",
            "Batch: 1500   D_Loss: 0.332   G_Loss: 2.162\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.289\n",
            "Batch: 1550   D_Loss: 0.330   G_Loss: 2.228\n",
            "Batch: 1575   D_Loss: 0.332   G_Loss: 2.213\n",
            "Batch: 1600   D_Loss: 0.330   G_Loss: 2.287\n",
            "Batch: 1625   D_Loss: 0.332   G_Loss: 2.241\n",
            "Batch: 1650   D_Loss: 0.344   G_Loss: 2.132\n",
            "Batch: 1675   D_Loss: 0.333   G_Loss: 2.204\n",
            "Batch: 1700   D_Loss: 0.333   G_Loss: 2.239\n",
            "Batch: 1725   D_Loss: 0.331   G_Loss: 2.230\n",
            "Batch: 1750   D_Loss: 0.332   G_Loss: 2.244\n",
            "Batch: 1775   D_Loss: 0.332   G_Loss: 2.282\n",
            "Batch: 1800   D_Loss: 0.330   G_Loss: 2.291\n",
            "Batch: 1825   D_Loss: 0.330   G_Loss: 2.291\n",
            "Batch: 1850   D_Loss: 0.331   G_Loss: 2.208\n",
            "Batch: 1875   D_Loss: 0.330   G_Loss: 2.266\n",
            "Epoch 14\n",
            "Batch: 25   D_Loss: 0.331   G_Loss: 2.251\n",
            "Batch: 50   D_Loss: 0.331   G_Loss: 2.314\n",
            "Batch: 75   D_Loss: 0.331   G_Loss: 2.279\n",
            "Batch: 100   D_Loss: 0.332   G_Loss: 2.288\n",
            "Batch: 125   D_Loss: 0.331   G_Loss: 2.231\n",
            "Batch: 150   D_Loss: 0.331   G_Loss: 2.281\n",
            "Batch: 175   D_Loss: 0.330   G_Loss: 2.298\n",
            "Batch: 200   D_Loss: 0.329   G_Loss: 2.271\n",
            "Batch: 225   D_Loss: 0.329   G_Loss: 2.309\n",
            "Batch: 250   D_Loss: 0.329   G_Loss: 2.273\n",
            "Batch: 275   D_Loss: 0.330   G_Loss: 2.253\n",
            "Batch: 300   D_Loss: 0.330   G_Loss: 2.245\n",
            "Batch: 325   D_Loss: 0.330   G_Loss: 2.301\n",
            "Batch: 350   D_Loss: 0.329   G_Loss: 2.271\n",
            "Batch: 375   D_Loss: 0.329   G_Loss: 2.278\n",
            "Batch: 400   D_Loss: 0.330   G_Loss: 2.276\n",
            "Batch: 425   D_Loss: 0.333   G_Loss: 2.247\n",
            "Batch: 450   D_Loss: 0.329   G_Loss: 2.267\n",
            "Batch: 475   D_Loss: 0.328   G_Loss: 2.266\n",
            "Batch: 500   D_Loss: 0.330   G_Loss: 2.268\n",
            "Batch: 525   D_Loss: 0.329   G_Loss: 2.256\n",
            "Batch: 550   D_Loss: 0.330   G_Loss: 2.306\n",
            "Batch: 575   D_Loss: 0.329   G_Loss: 2.302\n",
            "Batch: 600   D_Loss: 0.329   G_Loss: 2.265\n",
            "Batch: 625   D_Loss: 0.329   G_Loss: 2.274\n",
            "Batch: 650   D_Loss: 0.329   G_Loss: 2.312\n",
            "Batch: 675   D_Loss: 0.329   G_Loss: 2.308\n",
            "Batch: 700   D_Loss: 0.330   G_Loss: 2.284\n",
            "Batch: 725   D_Loss: 0.330   G_Loss: 2.266\n",
            "Batch: 750   D_Loss: 0.331   G_Loss: 2.310\n",
            "Batch: 775   D_Loss: 0.331   G_Loss: 2.302\n",
            "Batch: 800   D_Loss: 0.330   G_Loss: 2.294\n",
            "Batch: 825   D_Loss: 0.332   G_Loss: 2.254\n",
            "Batch: 850   D_Loss: 0.332   G_Loss: 2.274\n",
            "Batch: 875   D_Loss: 0.331   G_Loss: 2.241\n",
            "Batch: 900   D_Loss: 0.333   G_Loss: 2.241\n",
            "Batch: 925   D_Loss: 0.330   G_Loss: 2.276\n",
            "Batch: 950   D_Loss: 0.330   G_Loss: 2.278\n",
            "Batch: 975   D_Loss: 0.330   G_Loss: 2.295\n",
            "Batch: 1000   D_Loss: 0.331   G_Loss: 2.290\n",
            "Batch: 1025   D_Loss: 0.332   G_Loss: 2.234\n",
            "Batch: 1050   D_Loss: 0.330   G_Loss: 2.272\n",
            "Batch: 1075   D_Loss: 0.330   G_Loss: 2.290\n",
            "Batch: 1100   D_Loss: 0.331   G_Loss: 2.269\n",
            "Batch: 1125   D_Loss: 0.330   G_Loss: 2.303\n",
            "Batch: 1150   D_Loss: 0.332   G_Loss: 2.254\n",
            "Batch: 1175   D_Loss: 0.332   G_Loss: 2.248\n",
            "Batch: 1200   D_Loss: 0.331   G_Loss: 2.305\n",
            "Batch: 1225   D_Loss: 0.331   G_Loss: 2.219\n",
            "Batch: 1250   D_Loss: 0.333   G_Loss: 2.264\n",
            "Batch: 1275   D_Loss: 0.333   G_Loss: 2.254\n",
            "Batch: 1300   D_Loss: 0.334   G_Loss: 2.261\n",
            "Batch: 1325   D_Loss: 0.333   G_Loss: 2.250\n",
            "Batch: 1350   D_Loss: 0.330   G_Loss: 2.346\n",
            "Batch: 1375   D_Loss: 0.330   G_Loss: 2.272\n",
            "Batch: 1400   D_Loss: 0.332   G_Loss: 2.272\n",
            "Batch: 1425   D_Loss: 0.332   G_Loss: 2.276\n",
            "Batch: 1450   D_Loss: 0.331   G_Loss: 2.268\n",
            "Batch: 1475   D_Loss: 0.332   G_Loss: 2.260\n",
            "Batch: 1500   D_Loss: 0.332   G_Loss: 2.301\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.284\n",
            "Batch: 1550   D_Loss: 0.332   G_Loss: 2.261\n",
            "Batch: 1575   D_Loss: 0.330   G_Loss: 2.327\n",
            "Batch: 1600   D_Loss: 0.331   G_Loss: 2.285\n",
            "Batch: 1625   D_Loss: 0.329   G_Loss: 2.269\n",
            "Batch: 1650   D_Loss: 0.331   G_Loss: 2.274\n",
            "Batch: 1675   D_Loss: 0.330   G_Loss: 2.315\n",
            "Batch: 1700   D_Loss: 0.331   G_Loss: 2.252\n",
            "Batch: 1725   D_Loss: 0.330   G_Loss: 2.279\n",
            "Batch: 1750   D_Loss: 0.330   G_Loss: 2.268\n",
            "Batch: 1775   D_Loss: 0.332   G_Loss: 2.244\n",
            "Batch: 1800   D_Loss: 0.331   G_Loss: 2.240\n",
            "Batch: 1825   D_Loss: 0.331   G_Loss: 2.259\n",
            "Batch: 1850   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 1875   D_Loss: 0.332   G_Loss: 2.273\n",
            "Epoch 15\n",
            "Batch: 25   D_Loss: 0.330   G_Loss: 2.289\n",
            "Batch: 50   D_Loss: 0.330   G_Loss: 2.274\n",
            "Batch: 75   D_Loss: 0.330   G_Loss: 2.274\n",
            "Batch: 100   D_Loss: 0.329   G_Loss: 2.322\n",
            "Batch: 125   D_Loss: 0.330   G_Loss: 2.270\n",
            "Batch: 150   D_Loss: 0.329   G_Loss: 2.254\n",
            "Batch: 175   D_Loss: 0.329   G_Loss: 2.301\n",
            "Batch: 200   D_Loss: 0.330   G_Loss: 2.288\n",
            "Batch: 225   D_Loss: 0.330   G_Loss: 2.313\n",
            "Batch: 250   D_Loss: 0.329   G_Loss: 2.313\n",
            "Batch: 275   D_Loss: 0.330   G_Loss: 2.273\n",
            "Batch: 300   D_Loss: 0.331   G_Loss: 2.265\n",
            "Batch: 325   D_Loss: 0.330   G_Loss: 2.248\n",
            "Batch: 350   D_Loss: 0.330   G_Loss: 2.290\n",
            "Batch: 375   D_Loss: 0.331   G_Loss: 2.267\n",
            "Batch: 400   D_Loss: 0.330   G_Loss: 2.273\n",
            "Batch: 425   D_Loss: 0.331   G_Loss: 2.300\n",
            "Batch: 450   D_Loss: 0.330   G_Loss: 2.239\n",
            "Batch: 475   D_Loss: 0.330   G_Loss: 2.248\n",
            "Batch: 500   D_Loss: 0.332   G_Loss: 2.236\n",
            "Batch: 525   D_Loss: 0.330   G_Loss: 2.283\n",
            "Batch: 550   D_Loss: 0.329   G_Loss: 2.272\n",
            "Batch: 575   D_Loss: 0.329   G_Loss: 2.303\n",
            "Batch: 600   D_Loss: 0.329   G_Loss: 2.294\n",
            "Batch: 625   D_Loss: 0.329   G_Loss: 2.304\n",
            "Batch: 650   D_Loss: 0.331   G_Loss: 2.263\n",
            "Batch: 675   D_Loss: 0.332   G_Loss: 2.281\n",
            "Batch: 700   D_Loss: 0.330   G_Loss: 2.283\n",
            "Batch: 725   D_Loss: 0.331   G_Loss: 2.257\n",
            "Batch: 750   D_Loss: 0.329   G_Loss: 2.292\n",
            "Batch: 775   D_Loss: 0.334   G_Loss: 2.266\n",
            "Batch: 800   D_Loss: 0.333   G_Loss: 2.219\n",
            "Batch: 825   D_Loss: 0.330   G_Loss: 2.285\n",
            "Batch: 850   D_Loss: 0.331   G_Loss: 2.274\n",
            "Batch: 875   D_Loss: 0.330   G_Loss: 2.269\n",
            "Batch: 900   D_Loss: 0.330   G_Loss: 2.291\n",
            "Batch: 925   D_Loss: 0.329   G_Loss: 2.280\n",
            "Batch: 950   D_Loss: 0.329   G_Loss: 2.308\n",
            "Batch: 975   D_Loss: 0.331   G_Loss: 2.274\n",
            "Batch: 1000   D_Loss: 0.329   G_Loss: 2.277\n",
            "Batch: 1025   D_Loss: 0.329   G_Loss: 2.301\n",
            "Batch: 1050   D_Loss: 0.331   G_Loss: 2.301\n",
            "Batch: 1075   D_Loss: 0.329   G_Loss: 2.260\n",
            "Batch: 1100   D_Loss: 0.330   G_Loss: 2.288\n",
            "Batch: 1125   D_Loss: 0.329   G_Loss: 2.234\n",
            "Batch: 1150   D_Loss: 0.331   G_Loss: 2.281\n",
            "Batch: 1175   D_Loss: 0.330   G_Loss: 2.255\n",
            "Batch: 1200   D_Loss: 0.331   G_Loss: 2.213\n",
            "Batch: 1225   D_Loss: 0.329   G_Loss: 2.274\n",
            "Batch: 1250   D_Loss: 0.329   G_Loss: 2.261\n",
            "Batch: 1275   D_Loss: 0.329   G_Loss: 2.220\n",
            "Batch: 1300   D_Loss: 0.330   G_Loss: 2.193\n",
            "Batch: 1325   D_Loss: 0.329   G_Loss: 2.224\n",
            "Batch: 1350   D_Loss: 0.328   G_Loss: 2.256\n",
            "Batch: 1375   D_Loss: 0.329   G_Loss: 2.274\n",
            "Batch: 1400   D_Loss: 0.329   G_Loss: 2.240\n",
            "Batch: 1425   D_Loss: 0.330   G_Loss: 2.259\n",
            "Batch: 1450   D_Loss: 0.329   G_Loss: 2.242\n",
            "Batch: 1475   D_Loss: 0.330   G_Loss: 2.230\n",
            "Batch: 1500   D_Loss: 0.329   G_Loss: 2.258\n",
            "Batch: 1525   D_Loss: 0.331   G_Loss: 2.216\n",
            "Batch: 1550   D_Loss: 0.330   G_Loss: 2.260\n",
            "Batch: 1575   D_Loss: 0.329   G_Loss: 2.256\n",
            "Batch: 1600   D_Loss: 0.330   G_Loss: 2.265\n",
            "Batch: 1625   D_Loss: 0.330   G_Loss: 2.239\n",
            "Batch: 1650   D_Loss: 0.329   G_Loss: 2.297\n",
            "Batch: 1675   D_Loss: 0.329   G_Loss: 2.242\n",
            "Batch: 1700   D_Loss: 0.329   G_Loss: 2.297\n",
            "Batch: 1725   D_Loss: 0.329   G_Loss: 2.259\n",
            "Batch: 1750   D_Loss: 0.329   G_Loss: 2.261\n",
            "Batch: 1775   D_Loss: 0.329   G_Loss: 2.301\n",
            "Batch: 1800   D_Loss: 0.332   G_Loss: 2.233\n",
            "Batch: 1825   D_Loss: 0.330   G_Loss: 2.246\n",
            "Batch: 1850   D_Loss: 0.330   G_Loss: 2.306\n",
            "Batch: 1875   D_Loss: 0.330   G_Loss: 2.288\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('G.pt','rb') as f:\n",
        "  G.load_state_dict(torch.load(f))\n",
        "\n",
        "  z = torch.randn(32,latent_dim)\n",
        "\n",
        "  fake = G(z)\n",
        "\n",
        "  print(fake.shape)\n",
        "\n",
        "\n",
        "  fake_img = fake[0].detach().numpy()\n",
        "\n",
        "  fake_img = fake_img.transpose(1,2,0)\n",
        "  plt.imshow(fake_img, cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Fw-VSUdpYKb2",
        "outputId": "eb1ed541-87d3-46f8-9525-491932f2771b"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for Generator:\n\tsize mismatch for gen.4.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).\n\tsize mismatch for gen.7.weight: copying a param with shape torch.Size([64, 1, 2, 2]) from checkpoint, the shape in current model is torch.Size([64, 1, 4, 4]).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-132-88795cad20c2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'G.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Generator:\n\tsize mismatch for gen.4.weight: copying a param with shape torch.Size([128, 64, 2, 2]) from checkpoint, the shape in current model is torch.Size([128, 64, 4, 4]).\n\tsize mismatch for gen.7.weight: copying a param with shape torch.Size([64, 1, 2, 2]) from checkpoint, the shape in current model is torch.Size([64, 1, 4, 4])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJ1ehiAcKJx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "Z1GZAPPbeFw7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3af3ae-a029-4ee2-b67c-e07bb7eb058b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sys\n",
        "\n",
        "writer = SummaryWriter(\"runs\")"
      ],
      "metadata": {
        "id": "oSGGjek38gGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_grid = torchvision.utils.make_grid()\n",
        "writer.add_image(f'batch {batch+1} generated images:', img_grid)"
      ],
      "metadata": {
        "id": "ug0sdCGy8upD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}